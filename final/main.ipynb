{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "<span style=\"color:green; font-size:20px;\"> For testing the the whole code and UI inference, run cells that are marked with blue R. (**<span style=\"color:blue;\">R**) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8a9e5a03-53e4-4579-baf4-2cb3d450ebd6",
    "_uuid": "426652d8-b97a-4025-b550-817a457f44f9",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# Installing modules **<span style=\"color:blue;\">R**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "409d1032-e900-4e4d-a05b-77973680ad28",
    "_uuid": "a5b3b187-5000-4aeb-be9d-7fd28bdd4aad",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-18T10:01:58.525465Z",
     "iopub.status.busy": "2025-08-18T10:01:58.524947Z",
     "iopub.status.idle": "2025-08-18T10:03:36.105735Z",
     "shell.execute_reply": "2025-08-18T10:03:36.104947Z",
     "shell.execute_reply.started": "2025-08-18T10:01:58.525441Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m48.0/48.0 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25henv: EXA_API_KEY=af19a97b-45de-4ab4-8344-7029c5b7e7d6\n"
     ]
    }
   ],
   "source": [
    "# üîß 1) install modules\n",
    "!pip install --quiet transformers accelerate bitsandbytes sentence-transformers #faiss-cpu\n",
    "!pip install -q faiss-gpu-cu12\n",
    "!pip install -q \"pymupdf>=1.22\" #faiss-cpu \n",
    "\n",
    "!pip install -q exa-py\n",
    "!pip install -q python-dotenv\n",
    "%env EXA_API_KEY=\n",
    "EXA_ENDPOINT = \"https://api.exa.ai/search\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports **<span style=\"color:blue;\">R**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T10:03:42.997951Z",
     "iopub.status.busy": "2025-08-18T10:03:42.997660Z",
     "iopub.status.idle": "2025-08-18T10:04:13.454533Z",
     "shell.execute_reply": "2025-08-18T10:04:13.453755Z",
     "shell.execute_reply.started": "2025-08-18T10:03:42.997924Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 10:03:55.541785: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755511435.731399      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755511435.787345      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch, gc, os\n",
    "from huggingface_hub import login\n",
    "import json, time\n",
    "from collections import deque\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re   \n",
    "import glob, itertools, math, pathlib\n",
    "import fitz                           # PyMuPDF\n",
    "from pathlib import Path\n",
    "import requests, textwrap\n",
    "from exa_py import Exa\n",
    "import textwrap, random\n",
    "import gradio as gr\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "87a7ee58-9346-49a8-93a1-6d3c7a80d5e3",
    "_uuid": "5ca1ca4e-8355-427d-8b0b-a04098d309e9",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# Section 1 : Chat bot core "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:blue;font-size:25px\">R**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9b9f135c-7905-4e8b-8341-66c275f6c9a4",
    "_uuid": "4c9e0931-3bb3-40ef-bfb3-e5b4608228d9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-18T10:04:29.473646Z",
     "iopub.status.busy": "2025-08-18T10:04:29.472928Z",
     "iopub.status.idle": "2025-08-18T10:07:22.090574Z",
     "shell.execute_reply": "2025-08-18T10:07:22.089736Z",
     "shell.execute_reply.started": "2025-08-18T10:04:29.473616Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107735ca9d6543c3985e1c84cd4922ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/141k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa518e5becc94355a885f9636197883c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f878ec5e09224cccb2b2c13914f0d804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956eacea6db8438ea5d35b64209e73c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2e295eeeae43949570c54cba95ff22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1652d3c5aaab4adaa9f95d03b55e9e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28eb32d429840a7bb378c484d9ef8ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f347a5582db440839f3c3ee74d0a729d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2c2c1a4cff462aba37118978d42275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6febdc5b03bc428598703d8d1fc120af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43c244ca82a4a009a24a7270f4be665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506b7197809842cfab1241277ef47553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login(\"\")\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "bnb_cfg = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                             bnb_4bit_compute_dtype=torch.float16,\n",
    "                             bnb_4bit_use_double_quant=True,\n",
    "                             bnb_4bit_quant_type=\"nf4\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_cfg,\n",
    "    trust_remote_code=True,\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:blue;font-size:25px\">R**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "a70ecba6-4044-4f06-b82d-66613f0be0e8",
    "_uuid": "7ae813a2-07ad-4d82-aae7-6aa9951383ed",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-18T10:07:32.620155Z",
     "iopub.status.busy": "2025-08-18T10:07:32.619545Z",
     "iopub.status.idle": "2025-08-18T10:07:32.635805Z",
     "shell.execute_reply": "2025-08-18T10:07:32.635135Z",
     "shell.execute_reply.started": "2025-08-18T10:07:32.620131Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------ tweakables -----------------------------------------------------\n",
    "SYSTEM_PROMPT = \"You are a helpful AI assistant.\"\n",
    "MAX_CTX_TOKENS   = 8000 - 512        # keep 512 tokens headroom\n",
    "SUMMARISE_AT_TOK = 6000              # start summarising above this\n",
    "CHUNK_SIZE       = 12               # summarise 12 oldest turns each time\n",
    "LOG_FILE         = \"chatlog.jsonl\"   # optional disk log\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def num_tokens(text: str) -> int:\n",
    "    # helper for quick token counting\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def chat_completion(messages: List[Dict],  # messages[-1] must be user\n",
    "                    max_new=256, temp=0.7, top_p=0.9):\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new,\n",
    "        temperature=temp,\n",
    "        top_p=top_p,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    reply = tokenizer.decode(out[0][inputs.input_ids.shape[1]:],\n",
    "                             skip_special_tokens=True).strip()\n",
    "    return reply\n",
    "\n",
    "\n",
    "class MemoryChatbot:\n",
    "    \"\"\"Keeps the last N turns verbatim and auto-summarises earlier ones.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 system_prompt: str = SYSTEM_PROMPT,\n",
    "                 max_ctx_tokens: int = MAX_CTX_TOKENS,\n",
    "                 summarise_at: int = SUMMARISE_AT_TOK,\n",
    "                 chunk_size: int = CHUNK_SIZE):\n",
    "        self.system_prompt = system_prompt\n",
    "        self.max_ctx_tokens = max_ctx_tokens\n",
    "        self.summarise_at   = summarise_at\n",
    "        self.chunk_size     = chunk_size\n",
    "\n",
    "        self.history = deque()    # list of {\"role\":..., \"content\":...}\n",
    "        self.memo    = \"\"         # running summary of trimmed turns\n",
    "\n",
    "    # ------------- public API -------------------------------------------------\n",
    "    def ask(self, user_msg: str) -> str:\n",
    "        \"\"\"Main entry: add user message ‚Üí maybe summarise ‚Üí get reply.\"\"\"\n",
    "        self._append(\"user\", user_msg)\n",
    "        self._maybe_summarise()\n",
    "        reply = self._generate_reply(user_msg)\n",
    "        self._append(\"assistant\", reply)\n",
    "        return reply\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ------------- internal helpers ------------------------------------------\n",
    "    def _append(self, role, content):\n",
    "        self.history.append({\"role\": role, \"content\": content})\n",
    "        self._disk_log(role, content)\n",
    "\n",
    "    def _current_messages(self) -> List[Dict]:\n",
    "        sys = self.system_prompt\n",
    "        if self.memo:\n",
    "            sys = f\"{sys}\\n\\n[CONTEXT SUMMARY]\\n{self.memo}\"\n",
    "        msgs = [{\"role\": \"system\", \"content\": sys}]\n",
    "        msgs.extend(self.history)  # history starts with 'user'\n",
    "        return msgs\n",
    "        \n",
    "    def _prompt_tokens(self) -> int:\n",
    "        txt = tokenizer.apply_chat_template(self._current_messages(),\n",
    "                                            tokenize=False)\n",
    "        return num_tokens(txt)\n",
    "\n",
    "    def _maybe_summarise(self):\n",
    "        \"\"\"If conversation is getting heavy, summarise oldest chunk.\"\"\"\n",
    "        while self._prompt_tokens() > self.summarise_at and len(self.history) > self.chunk_size:\n",
    "            print(\"<<<Summarizing>>>\")\n",
    "            chunk = list(self.history)[:self.chunk_size]\n",
    "            summary = []\n",
    "            for i in range(0,len(chunk),2):\n",
    "                chunk_txt = f\"{chunk[i]['role']}: {chunk[i]['content']} \\n {chunk[i+1]['role']}: {chunk[i+1]['content']}\"\n",
    "                summary_prompt = [\n",
    "                    {\"role\": \"system\",\n",
    "                     \"content\": \"You are a summarisation assistant.\"},\n",
    "                    {\"role\": \"user\",\n",
    "                     \"content\":\n",
    "                     (\"Summarise the following conversation in one sentence, \"\n",
    "                      \"preserve all factual details:\\n\\n\" + chunk_txt)}\n",
    "                ]\n",
    "                summary.append(chat_completion(summary_prompt, max_new=30, temp=0.1))\n",
    "            \n",
    "            last_summary = \"\\n \".join(s for s in summary)\n",
    "            # remove chunk & prepend summary\n",
    "            for _ in range(self.chunk_size):\n",
    "                self.history.popleft()\n",
    "            if len(self.memo.split(\"\\n\")) > 40:\n",
    "                old_memo = self.memo.split(\"\\n\")\n",
    "                old_memo = \"\\n \".join(s for s in old_memo[15:])\n",
    "                self.memo = (old_memo + \"\\n\" + last_summary).strip()\n",
    "            else:\n",
    "                self.memo = (self.memo + \"\\n\" + last_summary).strip()\n",
    "            # free GPU RAM\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "            if self._prompt_tokens() < self.max_ctx_tokens:\n",
    "                break\n",
    "\n",
    "    def _generate_reply(self, user_msg):\n",
    "        msgs = self._current_messages()\n",
    "        reply = chat_completion(msgs)\n",
    "        gc.collect(); torch.cuda.empty_cache()\n",
    "        return reply\n",
    "\n",
    "    def _disk_log(self, role, content):\n",
    "        if LOG_FILE:\n",
    "            with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "                json.dump({\"ts\": time.time(), \"role\": role,\n",
    "                           \"content\": content}, f, ensure_ascii=False)\n",
    "                f.write(\"\\n\")\n",
    "    # -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "30111aba-760a-42df-b5e1-8428bb58e343",
    "_uuid": "ad3abb2f-cb53-4ff2-9f73-d82a44803298",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- quick demo ------------------------------------------\n",
    "bot = MemoryChatbot()\n",
    "\n",
    "print(\"ü§ñ How can I help you?\\n\")\n",
    "while True:\n",
    "    querry = input()\n",
    "    if querry == \"q\":\n",
    "        break\n",
    "    bot_ans = bot.ask(querry)\n",
    "    print(bot_ans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "85baec69-3c81-4bae-a6d2-02c2355c269a",
    "_uuid": "1d9e1e69-fff2-4af6-9ba9-4f72ccf5dc1c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# Section 2: RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-size:25px;\"> **Don't run this section for main code. This is just a prototype for test and RAG is already implemented in section 4.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "026965a5-f7f5-460a-90f9-051c98ab4f30",
    "_uuid": "0fc0ff74-69ff-4b61-b3cb-8fb833363945",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-11T14:19:08.714802Z",
     "iopub.status.busy": "2025-08-11T14:19:08.714469Z",
     "iopub.status.idle": "2025-08-11T14:19:08.731778Z",
     "shell.execute_reply": "2025-08-11T14:19:08.730797Z",
     "shell.execute_reply.started": "2025-08-11T14:19:08.714780Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index already present ‚Äì¬†skipping rebuild.\n"
     ]
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 1.  Build / load the FAISS index\n",
    "#     ‚Ä¢ Scans every *.pdf in /kaggle/input/pdf-folder\n",
    "#     ‚Ä¢ Extracts text with PyMuPDF\n",
    "#     ‚Ä¢ Splits it into ‚âà700‚Äëcharacter chunks\n",
    "#     ‚Ä¢ Embeds chunks with sentence‚Äëtransformers/all‚ÄëMiniLM‚ÄëL6‚Äëv2\n",
    "#     ‚Ä¢ Saves index + metadata to /kaggle/working for reuse\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "DATA_DIR   = \"/kaggle/input/pdf-folder\"\n",
    "INDEX_F    = \"/kaggle/working/rag.index\"\n",
    "META_F     = \"/kaggle/working/chunks.json\"\n",
    "CHUNK_SIZE = 700          # characters, ‚âà100‚ÄØwords\n",
    "\n",
    "def extract_text(pdf_path: str) -> str:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    return \"\\n\".join(page.get_text() for page in doc)\n",
    "\n",
    "def chunk_text(text: str, size: int = 700):\n",
    "    for start in range(0, len(text), size):\n",
    "        yield text[start : start + size]\n",
    "\n",
    "def clean(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalises the text so embeddings are not polluted by layout artefacts.\n",
    "\n",
    "    ‚Ä¢ replaces hard line‚Äëbreaks (\\n, \\r) with a single space\n",
    "    ‚Ä¢ keeps only ASCII letters, digits, and whitespace\n",
    "    ‚Ä¢ collapses 2+ whitespace chars into one space\n",
    "    \"\"\"\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    return text    \n",
    "\n",
    "def build_index():\n",
    "    pdf_files = sorted(glob.glob(os.path.join(DATA_DIR, \"*.pdf\")))\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(f\"No PDFs detected in {DATA_DIR}\")\n",
    "\n",
    "    chunks, meta = [], []\n",
    "    for path in pdf_files:\n",
    "        raw_text = extract_text(path)               # <-- original extractor\n",
    "        raw_text = clean(raw_text)                  # <-- NEW: sanitise once\n",
    "        for i, chunk in enumerate(chunk_text(raw_text, CHUNK_SIZE)):\n",
    "            meta.append({\"source\": os.path.basename(path), \"chunk_id\": i, \"text\": chunk})\n",
    "            chunks.append(chunk)\n",
    "\n",
    "    print(f\"‚úì Extracted {len(chunks)} chunks from {len(pdf_files)} file(s).\")\n",
    "\n",
    "    embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    vecs  = embed_model.encode(chunks, batch_size=32, show_progress_bar=True).astype(\"float32\")\n",
    "\n",
    "    index = faiss.IndexFlatL2(vecs.shape[1])\n",
    "    index.add(vecs)\n",
    "\n",
    "    faiss.write_index(index, INDEX_F)\n",
    "    with open(META_F, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úì Index saved to {INDEX_F}; metadata to {META_F}\")\n",
    "\n",
    "# Build only if we have not done so already\n",
    "if not (pathlib.Path(INDEX_F).exists() and pathlib.Path(META_F).exists()):\n",
    "    build_index()\n",
    "else:\n",
    "    print(\"Index already present ‚Äì¬†skipping rebuild.\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 2.  Lightweight retriever class (Section‚ÄØ2)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "class Retriever:\n",
    "    \"\"\"\n",
    "    ‚Ä¢ Filters out FAISS ‚Äòempty‚Äëslot‚Äô returns (id¬†==¬†‚Äë1, distance¬†==¬†FLT_MAX)\n",
    "    ‚Ä¢ Converts L2 distance to cosine‚Äëlike similarity in [0,‚ÄØ1]\n",
    "    \"\"\"\n",
    "    def __init__(self, idx_path=\"/kaggle/working/rag.index\",\n",
    "                       meta_path=\"/kaggle/working/chunks.json\"):\n",
    "        self.model  = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        self.index  = faiss.read_index(idx_path)\n",
    "        with open(meta_path, encoding=\"utf-8\") as f:\n",
    "            self.meta = json.load(f)\n",
    "        # pre‚Äëcompute norms once for the conversion formula\n",
    "        self._vec_norm = np.linalg.norm(\n",
    "            self.index.reconstruct(0) ).astype(\"float32\")  # all vectors same length\n",
    "\n",
    "    def _l2_to_similarity(self, l2: float) -> float:\n",
    "        # cosine_sim = 1 - (L2_dist¬≤) / (2 * |a|¬≤)   for unit‚Äëlength queries ‚âà 1\n",
    "        return max(0.0, 1.0 - l2 / (2 * self._vec_norm**2))\n",
    "\n",
    "    def top_k(self, query: str, k: int = 3):\n",
    "        q_vec  = self.model.encode([query]).astype(\"float32\")\n",
    "        D, I   = self.index.search(q_vec, k)\n",
    "        hits   = []\n",
    "        for rank, (idx, dist) in enumerate(zip(I[0], D[0])):\n",
    "            if idx == -1 or np.isinf(dist) or dist > 1e8:\n",
    "                continue                          # FAISS padding ‚Üí skip\n",
    "            hits.append({\n",
    "                \"rank\"     : rank + 1,\n",
    "                \"source\"   : self.meta[idx][\"source\"],\n",
    "                \"chunk_id\" : self.meta[idx][\"chunk_id\"],\n",
    "                \"similarity\": round(self._l2_to_similarity(dist), 3),\n",
    "                \"text\"     : self.meta[idx][\"text\"].strip()\n",
    "            })\n",
    "        return hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "c2667441-be5c-4aee-8269-3b6ccc634202",
    "_uuid": "150e013e-2cec-4eaf-a738-93d1c2d67479",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-11T14:19:13.137578Z",
     "iopub.status.busy": "2025-08-11T14:19:13.137276Z",
     "iopub.status.idle": "2025-08-11T14:19:14.297758Z",
     "shell.execute_reply": "2025-08-11T14:19:14.296904Z",
     "shell.execute_reply.started": "2025-08-11T14:19:13.137534Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# make sure the index was built\n",
    "assert Path(\"/kaggle/working/rag.index\").exists(), \"Run the build‚Äëindex cell first.\"\n",
    "\n",
    "retriever_first = Retriever(                       # ‚Üê class from Section‚ÄØ2\n",
    "    idx_path = \"/kaggle/working/rag.index\",\n",
    "    meta_path = \"/kaggle/working/chunks.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "08660a47-6850-4565-8e3c-6a8720ff06d8",
    "_uuid": "610395c4-be89-4fac-9db3-231c5dd9c1da",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-11T14:19:16.533376Z",
     "iopub.status.busy": "2025-08-11T14:19:16.533077Z",
     "iopub.status.idle": "2025-08-11T14:19:16.540973Z",
     "shell.execute_reply": "2025-08-11T14:19:16.540056Z",
     "shell.execute_reply.started": "2025-08-11T14:19:16.533356Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RAGMemoryChatbot(MemoryChatbot):\n",
    "    \"\"\"\n",
    "    Adds Retrieval‚ÄëAugmented Generation (RAG) on top of the Section‚ÄØ1 bot.\n",
    "    History handling, summarisation, and disk logging remain untouched.\n",
    "    \"\"\"\n",
    "    def __init__(self, retriever, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.retriever = retriever\n",
    "        \n",
    "    def ask(self, user_msg: str, k: int = 3) -> str:\n",
    "        # 1) keep the normal bookkeeping\n",
    "        self._append(\"user\", user_msg)\n",
    "        self._maybe_summarise()\n",
    "\n",
    "        # 2) fetch top‚Äëk supporting passages (if any)\n",
    "        rag_block = \"\"\n",
    "        if self.retriever is not None:\n",
    "            hits = self.retriever.top_k(user_msg, k=k)\n",
    "            if hits:\n",
    "                rag_block = (\"\\n\\n---\\n[Local Doc]\\n Relevant background:\\n\" +\n",
    "                             \"  \".join(f\" {h['text']}\" for h in hits) +\n",
    "                             \"\\n---\")\n",
    "\n",
    "        # 3) build the prompt stack with the extra context\n",
    "        messages = self._current_messages()             # system + history\n",
    "        messages[0][\"content\"] += rag_block             # add to system prompt\n",
    "        # 4) call exactly the same LLM wrapper you already use\n",
    "        reply = chat_completion(messages,temp = 0.2)\n",
    "        \n",
    "        # 5) store assistant reply and continue as before\n",
    "        self._append(\"assistant\", reply)\n",
    "        return reply\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "8dfa9bea-10e9-40d6-b491-b19ecb92b575",
    "_uuid": "32ef8944-693e-41c9-bddb-4003a8384bff",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-08T18:14:12.335627Z",
     "iopub.status.busy": "2025-08-08T18:14:12.335042Z",
     "iopub.status.idle": "2025-08-08T18:14:25.905026Z",
     "shell.execute_reply": "2025-08-08T18:14:25.904481Z",
     "shell.execute_reply.started": "2025-08-08T18:14:12.335596Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ How can I help you?\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " which breeds are mentioned in doc?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a643ae551bd4aa6bffca4a53d25f2e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document mentions the following cat breeds: Persian, Siamese, Maine Coon, and Sphynx.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " q\n"
     ]
    }
   ],
   "source": [
    "# ----------------------- quick demo ------------------------------------------\n",
    "bot = RAGMemoryChatbot(retriever = retriever_first)\n",
    "\n",
    "print(\"ü§ñ How can I help you?\\n\")\n",
    "while True:\n",
    "    querry = input()\n",
    "    if querry == \"q\":\n",
    "        break\n",
    "    bot_ans = bot.ask(querry)\n",
    "    print(bot_ans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f067c4ac-833b-4eb6-8a2d-439e909a281e",
    "_uuid": "ee2b4945-ec1f-4e40-a373-dbae8dc5fceb",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# Section 3 : Function call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f4066152-a457-4c0d-b272-aa722a0f586a",
    "_uuid": "f0635ccf-7d78-44ce-9588-84405c0358e6",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "<span style=\"font-size:20px;\"> Part 3.1 (searching the web) **<span style=\"color:blue;font-size:25px\">R**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "1e89a974-55e0-456c-9576-ad462c9bb42d",
    "_uuid": "e1ad39e8-9344-4f34-a3e5-d1984835426d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-18T10:07:47.003964Z",
     "iopub.status.busy": "2025-08-18T10:07:47.003681Z",
     "iopub.status.idle": "2025-08-18T10:07:47.015724Z",
     "shell.execute_reply": "2025-08-18T10:07:47.015105Z",
     "shell.execute_reply.started": "2025-08-18T10:07:47.003942Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Section 3 ‚Äì Web-augmented RAG + Memory\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "class WebRAGMemoryChatbot(MemoryChatbot):\n",
    "    \"\"\"\n",
    "    RAG + Memory + web trigger (Part 3.1)\n",
    "    --------------------------------------------------------------------------\n",
    "    ‚Ä¢ inherits history, summarisation, and local-PDF RAG from Section 2\n",
    "    ‚Ä¢ adds:\n",
    "        ‚Äì _needs_web_search   ‚Üí yes/no (with LLM)\n",
    "        ‚Äì _exa_search         ‚Üí 3 hits  (Exa API)\n",
    "        ‚Äì _summarise_hits     ‚Üí ‚â§120-word digest\n",
    "    \"\"\"\n",
    "    # ---------- init ---------------------------------------------------------\n",
    "    def __init__(self, retriever , exa_api_key: str, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)         # from RAGMemoryChatbot\n",
    "        self.exa_api_key = exa_api_key\n",
    "        self.exa = Exa(exa_api_key)   \n",
    "        self.retriever = retriever\n",
    "    # ---------- helpers ------------------------------------------------------        \n",
    "    def _needs_web_search(self, user_msg: str) -> bool:\n",
    "        \"\"\"\n",
    "        Upgraded classifier to determine if a web search is necessary.\n",
    "        \"\"\"\n",
    "        \n",
    "        # The new, more descriptive prompt with criteria and examples\n",
    "        prompt = f'''\n",
    "        You are an expert classifier. Your task is to determine if a user's question requires a real-time web search.\n",
    "        Reply with a single word: YES or NO.\n",
    "        \n",
    "        ## CRITERIA\n",
    "        You must answer YES if the question asks for:\n",
    "        - Current events or news (e.g., \"what happened in France today?\")\n",
    "        - Real-time information (e.g., \"what's the price of gold?\", \"what's the weather in London?\")\n",
    "        - Information about a very recent topic or public figure.\n",
    "        \n",
    "        You must answer NO if the question is about:\n",
    "        - General knowledge (e.g., \"what is the capital of Japan?\")\n",
    "        - Math, logic, or creative writing.\n",
    "        - Information contained within provided documents.\n",
    "        - A greeting or a question about your identity.\n",
    "        \n",
    "        ## EXAMPLES\n",
    "        User: What date is today?\n",
    "        Assistant: YES\n",
    "        \n",
    "        User: What is the weather like in Tehran today?\n",
    "        Assistant: YES\n",
    "        \n",
    "        User: Can you tell me a story about a dragon?\n",
    "        Assistant: NO\n",
    "        \n",
    "        User: what news are about Syria today?\n",
    "        Assistant: YES\n",
    "        \n",
    "        User: what is information about persian cat in doc?\n",
    "        Assistant: NO\n",
    "        \n",
    "        ## TASK\n",
    "        Now, classify the following user question. Remember to only reply with YES or NO.\n",
    "        \n",
    "        User: {user_msg}\n",
    "        Assistant:\n",
    "        '''\n",
    "    \n",
    "        messages = [\n",
    "            # The detailed instructions are now in the user message for better focus\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        # Increase max_new_tokens to safely generate \"yes\" or \"no\"\n",
    "        # Lower temperature for more stable, deterministic classification\n",
    "        out = chat_completion(messages, max_new=4, temp=0.1).strip().lower()\n",
    "        # Check for a clean \"yes\"\n",
    "        return out.startswith(\"yes\")\n",
    "\n",
    "    def _exa_search(self, query: str, k: int = 3):\n",
    "        \"\"\"\n",
    "        Uses exa_py.search_and_contents ‚Üí one call does both search + content\n",
    "        Returns a list[{title,url,summary}] for downstream summarisation.\n",
    "        \"\"\"\n",
    "        if not self.exa_api_key:\n",
    "            raise ValueError(\"‚ùå EXA_API_KEY is empty or not set.\")\n",
    "    \n",
    "        # call the SDK ‚Äì we want full page text, not just metadata\n",
    "        response = self.exa.search_and_contents(\n",
    "            query,\n",
    "            text=True,                # full text of each result\n",
    "            num_results=k,             # ‚Üê snake_case in SDK\n",
    "            # contex = True,\n",
    "            summary = True\n",
    "        )                              # :contentReference[oaicite:0]{index=0}\n",
    "        hits = []\n",
    "        for r in response.results:     # ResultWithText objects\n",
    "            hits.append({\n",
    "                \"title\":   r.title,\n",
    "                \"url\":     r.url,\n",
    "                \"text\": (r.text or \"\")#[:600]      # first 400 chars\n",
    "                ,\"summary\": r.summary\n",
    "            })\n",
    "            \n",
    "        return hits\n",
    "\n",
    "\n",
    "    def _summarise_hits(self, hits) -> str:\n",
    "        joined = \"\\n\".join(f\"‚Ä¢ {h['title']}: {h['summary']}\" for h in hits)\n",
    "        prompt = [\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": \"Summarise the following web snippets in ‚â§120 words.\"},\n",
    "            {\"role\": \"user\", \"content\": joined}\n",
    "        ]\n",
    "        return chat_completion(prompt, max_new=160, temp=0.3).strip()\n",
    "\n",
    "\n",
    "    # ---------- public API ---------------------------------------------------\n",
    "    def ask(self, user_msg: str, k: int = 3) -> str:\n",
    "        # 1) bookkeeping + maybe summarise\n",
    "        self._append(\"user\", user_msg)\n",
    "        self._maybe_summarise()\n",
    "        # 2) RAG (PDF)\n",
    "        rag_block = \"\"\n",
    "        if self.retriever is not None:\n",
    "            hits = self.retriever.top_k(user_msg, k=k)\n",
    "            if hits:\n",
    "                rag_block = (\"\\n\\n---\\n[Local Doc]\\n Relevant background:\\n\" +\n",
    "                             \"  \".join(f\" {h['text']}\" for h in hits) +\n",
    "                             \"\\n---\")\n",
    "\n",
    "        # 3) Web-Trigger\n",
    "        web_block = \"\"\n",
    "        summaries = \"\"\n",
    "        if self._needs_web_search(user_msg):\n",
    "            try:\n",
    "                web_hits = self._exa_search(user_msg, k=3)\n",
    "                # summary  = self._summarise_hits(web_hits)\n",
    "                for h in web_hits:\n",
    "                    summaries+= h[\"summary\"] \n",
    "                web_block = f\"\\n\\n---\\n[Web Info]\\n{summaries}\\n---\"\n",
    "                print(\"Web search done!\\n\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(\"‚ö†Ô∏è  Web search failed:\", e)\n",
    "        \n",
    "\n",
    "        # 4) build full prompt & get answer\n",
    "        messages = self._current_messages()        # system + history (+memo)\n",
    "        messages[0][\"content\"] +=  rag_block + web_block \n",
    "        reply = chat_completion(messages,max_new=512, temp=0.5)\n",
    "        \n",
    "        # 5) save reply\n",
    "        self._append(\"assistant\", reply)\n",
    "        return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "446548ca-5378-45a0-a1e8-d3fcd4aa6665",
    "_uuid": "d8df346a-27df-4b19-82ed-9e38e965582c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ----------------------- quick demo ------------------------------------------\n",
    "bot = WebRAGMemoryChatbot(\n",
    "        exa_api_key=os.getenv(\"EXA_API_KEY\"),\n",
    "        retriever=retriever_first,\n",
    "        system_prompt=SYSTEM_PROMPT\n",
    ")\n",
    "print(\"ü§ñ How can I help you?\\n\")\n",
    "while True:\n",
    "    querry = input()\n",
    "    if querry == \"q\":\n",
    "        break\n",
    "    bot_ans = bot.ask(querry)\n",
    "    print(bot_ans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4f2184b5-9f51-4420-bcb3-df2ae413ba79",
    "_uuid": "f55d95ff-e469-4edd-af28-bc711a39450f",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "<span style=\"font-size:20px;\"> Part 3.2 (Implementing 20_Question) **<span style=\"color:blue;font-size:25px\">R**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "154a96b8-b418-4a52-9a42-3a9964235ccd",
    "_uuid": "a262c99a-5761-45ee-8c41-7b1810990654",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-18T10:07:52.975529Z",
     "iopub.status.busy": "2025-08-18T10:07:52.974918Z",
     "iopub.status.idle": "2025-08-18T10:07:52.992576Z",
     "shell.execute_reply": "2025-08-18T10:07:52.991891Z",
     "shell.execute_reply.started": "2025-08-18T10:07:52.975505Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Game-aware Web-RAG + Memory Chatbot  (20-Questions, EN-only)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "class GameWebRAGMemoryChatbot(WebRAGMemoryChatbot):\n",
    "    # --------------------------------------------------------------------- #\n",
    "    # 0. constructor & single-point game-state init\n",
    "    # --------------------------------------------------------------------- #\n",
    "    def __init__(self, retriever , exa_api_key: str,rt, *args, **kwargs):\n",
    "        super().__init__(retriever,exa_api_key, *args, **kwargs)\n",
    "        self._init_game_state()\n",
    "        self.return_type = rt\n",
    "    \n",
    "    def _init_game_state(self):\n",
    "        self.game_active     = False\n",
    "        self.phase           = \"ask\"     # \"ask\" | \"guess\"\n",
    "        self.questions_asked = 0\n",
    "        self.max_questions   = 20\n",
    "        self.history_qna     = []        # [(question, answer), ‚Ä¶]\n",
    "        self._asked_set      = set()     # lower-cased questions\n",
    "        self._guess_set      = set()     # lower-cased guesses\n",
    "        self.question_num = 0\n",
    "    # --------------------------------------------------------------------- #\n",
    "    # 1. intent detection helpers\n",
    "    # --------------------------------------------------------------------- #\n",
    "    def _detect_game_start(self, user_msg: str) -> bool:\n",
    "        prompt = [\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": (\"You are a classifier. Respond with *exactly* YES or NO \"\n",
    "                         \"(uppercase, no punctuation) to the following question:\\n\"\n",
    "                         \"Does the user explicitly ask to PLAY a 20-Questions game?\\n\"\n",
    "                         \"If they only greet, ask who you are, or anything else, answer NO.\")},\n",
    "            {\"role\": \"user\", \"content\": user_msg.strip()}\n",
    "        ]\n",
    "        reply = chat_completion(prompt,\n",
    "                            max_new=1,\n",
    "                            temp=0.0,\n",
    "                            top_p=0.0)\n",
    "        \n",
    "        return reply.strip().lower().startswith(\"y\")\n",
    "\n",
    "    def _detect_quit(self, user_msg: str) -> bool:\n",
    "        return bool(re.search(r\"\\b(quit|exit|cancel|stop|end|end game)\\b\",\n",
    "                              user_msg, flags=re.I))\n",
    "        \n",
    "    # --------------------------------------------------------------------- #\n",
    "    # 2. guess and question generator \n",
    "    # --------------------------------------------------------------------- #\n",
    "    \n",
    "    def _generate_question(self) -> str:\n",
    "       # ------------------------------------------------------------------ #\n",
    "        # 1) dynamic LLM-generated question\n",
    "        # ------------------------------------------------------------------ #\n",
    "        history_summary = \"\\\\n\".join([f\"Q: {q} | A: {a}\" for q, a in self.history_qna])\n",
    "        asked_questions = \"\\\\n\".join(sorted(self._asked_set))\n",
    "    \n",
    "        system_prompt = (\n",
    "            \"You are playing a game of 20 Questions. Your task is to ask a strategic and simple 'yes' or 'no' question to help you guess the secret object. The question must be new and not a repeat of one you have asked before.\"\n",
    "        )\n",
    "        user_prompt = (\n",
    "            f\"Here is the conversation history so far:\\\\n{history_summary}\\\\n\\\\n\"\n",
    "            f\"Here are the questions you have already asked:\\\\n{asked_questions}\\\\n\\\\n\"\n",
    "            \"Based on the history, generate the next logical and simple 'yes' or 'no' question to ask. The question should be concise and clear (under 12 words).\\\\n\\\\n\"\n",
    "            \"**IMPORTANT RULES:**\\\\n\"\n",
    "            \"- The question MUST be answerable with a simple 'yes' or 'no'.\\\\n\"\n",
    "            \"- DO NOT ask questions that contain the word 'or' (e.g., 'Is it a liquid or a gas?').\\\\n\"\n",
    "            \"- DO NOT ask for a guess.\\\\n\\\\n\"\n",
    "            \"Generate only the question.\"\n",
    "        )\n",
    "\n",
    "        prompt = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                  {\"role\": \"user\",   \"content\": user_prompt}]\n",
    "    \n",
    "        candidate = chat_completion(prompt, max_new=20, temp=0.4).strip()\n",
    "        # Clean up the generated question\n",
    "        candidate = candidate.split(\"?\")[0].strip().capitalize() + \"?\"\n",
    "        if (2 <= len(candidate.split()) <= 20\n",
    "                and candidate.lower() not in self._asked_set):\n",
    "            self._asked_set.add(candidate.lower())\n",
    "            return candidate\n",
    "    \n",
    "        # ------------------------------------------------------------------ #\n",
    "        # 2) ultimate static fall-back\n",
    "        # ------------------------------------------------------------------ #\n",
    "        FALLBACKS = [\n",
    "            \"Is it man-made?\",\n",
    "            \"Is it bigger than a loaf of bread?\",\n",
    "            \"Is it commonly found indoors?\",\n",
    "            \"Is it electronic?\",\n",
    "        ]\n",
    "        for fb in FALLBACKS:\n",
    "            fb_l = fb.lower()\n",
    "            if fb_l not in self._asked_set:\n",
    "                self._asked_set.add(fb_l)\n",
    "                return fb\n",
    "    \n",
    "        # If absolutely everything else fails\n",
    "        return \"Is it tangible?\"\n",
    "\n",
    "    def _generate_guess(self) -> str:\n",
    "    \n",
    "        history_summary = \"\\\\n\".join([f\"Q: {q} | A: {a}\" for q, a in self.history_qna])\n",
    "        forbidden_guesses = \", \".join(self._guess_set)\n",
    "        \n",
    "        system_prompt = (\n",
    "            \"You are playing a game of 20 Questions. Your task is to guess the secret item. Based on the provided history of questions and answers, your goal is to provide a single, concrete noun as your guess. Do not provide any explanation or surrounding text.\"\n",
    "        )\n",
    "        user_prompt = (\n",
    "            f\"Here is the history of the game so far:\\\\n{history_summary}\\\\n\\\\n\"\n",
    "            f\"You have already guessed the following words: {forbidden_guesses}\\\\n\\\\n\"\n",
    "            \"Based on this information, what is your single-word guess for the secret item? Your response must be a single word and a concrete noun.\"\n",
    "        )\n",
    "        prompt = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                  {\"role\": \"user\",   \"content\": user_prompt or \"[start]\"}]\n",
    "    \n",
    "        g = chat_completion(prompt, max_new=4, temp=0.0).strip()\n",
    "        # Take the first word if multiple are generated\n",
    "        g = g.split()[0]\n",
    "        g = re.sub(r\"[^a-zA-Z]\", \"\", g).lower()\n",
    "        BAD_GUESSES = {\"\", \"thing\", \"object\", \"based\",\"i\",\"my\"}\n",
    "        if g and len(g) > 1 and g not in self._guess_set and g not in BAD_GUESSES:\n",
    "            return g\n",
    "            \n",
    "        GUESS_FALLBACKS = [\"cat\", \"dog\", \"tree\", \"car\", \"phone\", \"apple\", \"lion\"]\n",
    "\n",
    "        for fb in GUESS_FALLBACKS:\n",
    "            if fb not in self._guess_set:\n",
    "                return fb\n",
    "        return \"idea\"\n",
    "        \n",
    "    # --------------------------------------------------------------------- #\n",
    "    # 3. public ask()  (state machine)\n",
    "    # --------------------------------------------------------------------- #\n",
    "    def ask(self, user_msg: str, k: int = 3) -> str:\n",
    "        # -------- escape hatch --------\n",
    "        if self.game_active and self._detect_quit(user_msg):\n",
    "            self._init_game_state()\n",
    "            return \"Game stopped. Say 'play 20 questions' to start again.\"\n",
    "        # -------- active game ---------\n",
    "        if self.game_active:\n",
    "            # user answered a QUESTION  ‚Üí now make a GUESS\n",
    "            if self.phase == \"ask\":\n",
    "                self.history_qna[-1] = (self.history_qna[-1][0], user_msg.strip())\n",
    "                guess = self._generate_guess()\n",
    "                self._guess_set.add(guess.lower())\n",
    "                self.phase = \"guess\"\n",
    "                # return f\"Is it **{guess}**? (Yes/No)\"\n",
    "                if self.return_type:\n",
    "                    return \"Guess: \" + guess.lower() + \" (Yes/No)\"\n",
    "                else:\n",
    "                    return guess.lower()\n",
    "            # user judged our GUESS\n",
    "            if self.phase == \"guess\":\n",
    "                if user_msg.lower().startswith(\"yes\"):\n",
    "                    self._init_game_state()\n",
    "                    return \"üéâ I guessed it! Thanks for playing.\"\n",
    "                # wrong guess\n",
    "                self.questions_asked += 1\n",
    "                if self.questions_asked >= self.max_questions:\n",
    "                    self._init_game_state()\n",
    "                    return \"üòî I couldn't get it in 20 tries. You win!\"\n",
    "                # next question\n",
    "                self.question_num += 1\n",
    "                q = self._generate_question()\n",
    "                self.history_qna.append((q, None))\n",
    "                self._asked_set.add(q.lower())\n",
    "                self.phase = \"ask\"\n",
    "                if self.return_type:\n",
    "                    return str(self.question_num) + '. ' + q + \" (Yes/No)\"\n",
    "                else:\n",
    "                    return q + \" (Yes/No)\"\n",
    "        # -------- start trigger -------\n",
    "        if self._detect_game_start(user_msg):\n",
    "            self._init_game_state()\n",
    "            self.game_active = True\n",
    "            self.question_num += 1\n",
    "            first_q = self._generate_question()\n",
    "            self.history_qna.append((first_q, None))\n",
    "            self._asked_set.add(first_q.lower())\n",
    "            intro = (\"Let's play 20 Questions! Think of a word; \"\n",
    "                     \"I'll guess in ‚â§20 yes/no questions.\")\n",
    "            if self.return_type:\n",
    "                return intro + \"\\n\\n\" + str(self.question_num) + '. ' +first_q + \" (Yes/No)\"\n",
    "            else:\n",
    "                return first_q + \" (Yes/No)\"\n",
    "        # -------- normal chat ---------\n",
    "        return super().ask(user_msg, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "52203d39-dfe5-4208-bf00-1915a7b45826",
    "_uuid": "e5b888c0-e11f-4627-9962-9dcb6731736d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-17T21:55:15.382198Z",
     "iopub.status.busy": "2025-08-17T21:55:15.381558Z",
     "iopub.status.idle": "2025-08-17T21:55:32.183597Z",
     "shell.execute_reply": "2025-08-17T21:55:32.182956Z",
     "shell.execute_reply.started": "2025-08-17T21:55:15.382173Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's play 20 Questions! Think of a word; I'll guess in ‚â§20 yes/no questions.\n",
      "\n",
      "1. Is it animate? (Yes/No)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is your answer? yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guess: person (Yes/No)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is your answer? no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Is it a human or an animal? (Yes/No)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is your answer? yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guess: dog (Yes/No)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is your answer? yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ I guessed it! Thanks for playing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is your answer? q\n"
     ]
    }
   ],
   "source": [
    "bot = GameWebRAGMemoryChatbot(\n",
    "        exa_api_key=os.getenv(\"EXA_API_KEY\"),\n",
    "        retriever=None,\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        rt = 1\n",
    ")\n",
    "\n",
    "user_ans = \"I wanna play 20 question.\"\n",
    "querries_0 = []\n",
    "i = 0\n",
    "while user_ans != \"q\":\n",
    "    model_ques = bot.ask(user_ans)\n",
    "    print(model_ques)\n",
    "    user_ans = input(\"what is your answer?\")\n",
    "    \n",
    "    querries_0.append({\"Model Question\":model_ques,\"User answer\":user_ans})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c1a23bf5-a66b-49e5-8472-105160aad27d",
    "_uuid": "ee6678a8-5d2a-4fc1-bd80-7e2c7d62a1ff",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "bot = GameWebRAGMemoryChatbot(\n",
    "        exa_api_key=os.getenv(\"EXA_API_KEY\"),\n",
    "        retriever=retriever_first,\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "         rt = 1\n",
    ")\n",
    "\n",
    "user_ans = input(\"ü§ñ How can I help you?\\n\")\n",
    "querries_zero = []\n",
    "\n",
    "while user_ans != \"q\":\n",
    "    model_ans = bot.ask(user_ans)\n",
    "    print(model_ans)\n",
    "    user_ans = input()\n",
    "\n",
    "    querries_zero.append({\"Model Question\":model_ans,\"User answer\":user_ans})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ef63b26d-f376-4e8b-9ae1-fb7a7630b5c1",
    "_uuid": "44b736af-4c9a-47b5-9c81-f263d0468df4",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "<span style=\"font-size:20px;\"> **Validator**\n",
    "\n",
    "Below code write a python file that consist of ValidatorModel class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "804bd945-02ec-40a7-a6f0-b7cfe9260c7c",
    "_uuid": "4917be35-c5f2-4fb4-a38e-ff2098ba9c2d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-18T10:26:22.508425Z",
     "iopub.status.busy": "2025-08-18T10:26:22.508126Z",
     "iopub.status.idle": "2025-08-18T10:26:22.518803Z",
     "shell.execute_reply": "2025-08-18T10:26:22.517864Z",
     "shell.execute_reply.started": "2025-08-18T10:26:22.508400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing validator_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile validator_model.py\n",
    "\n",
    "#===== part1 =====\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch, gc, os\n",
    "from huggingface_hub import login\n",
    "import json, gc, os, time\n",
    "from collections import deque\n",
    "from typing import List, Dict\n",
    "\n",
    "login(\"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\")\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "bnb_cfg = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                             bnb_4bit_compute_dtype=torch.float16,\n",
    "                             bnb_4bit_use_double_quant=True,\n",
    "                             bnb_4bit_quant_type=\"nf4\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_cfg,\n",
    "    trust_remote_code=True,\n",
    ").eval()\n",
    "\n",
    "# ==== part2 ====\n",
    "\n",
    "def Ask_AI(messages: List[Dict],  # messages[-1] must be user\n",
    "                    max_new=256, temp=0.7, top_p=0.9):\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new,\n",
    "        temperature=temp,\n",
    "        top_p=top_p,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    reply = tokenizer.decode(out[0][inputs.input_ids.shape[1]:],\n",
    "                             skip_special_tokens=True).strip()\n",
    "    return reply\n",
    "\n",
    "# ===== part3 =====\n",
    "\n",
    "game_words = [\n",
    "    \"cat\", \"dog\", \"cow\", \"horse\", \"rabbit\", \"lion\", \"bear\", \"shark\", \"eagle\", \"ant\",\n",
    "    \"apple\", \"banana\", \"orange\", \"carrot\", \"bread\", \"cheese\", \"pizza\", \"cookie\", \"egg\", \"ice-cream\",\n",
    "    \"chair\", \"table\", \"sofa\", \"bed\", \"lamp\", \"clock\", \"mirror\", \"door\", \"window\", \"carpet\",\n",
    "    \"car\", \"bicycle\", \"bus\", \"train\", \"airplane\", \"boat\", \"rocket\", \"helmet\", \"engine\", \"wheel\",\n",
    "    \"pencil\", \"pen\", \"book\", \"paper\", \"scissors\", \"ruler\", \"eraser\", \"backpack\", \"laptop\", \"phone\",\n",
    "    \"ball\", \"doll\", \"puzzle\", \"kite\", \"yo-yo\", \"drum\", \"guitar\", \"camera\", \"radio\", \"television\",\n",
    "    \"shirt\", \"pants\", \"jacket\", \"hat\", \"shoes\", \"gloves\", \"umbrella\", \"wallet\", \"watch\", \"glasses\",\n",
    "    \"moon\", \"sun\", \"star\", \"cloud\", \"rain\", \"snow\", \"mountain\", \"river\", \"ocean\", \"island\",\n",
    "    \"doctor\", \"teacher\", \"chef\", \"farmer\", \"artist\", \"pilot\", \"police\", \"firefighter\", \"singer\", \"dancer\",\n",
    "    \"gold\", \"silver\", \"iron\", \"sand\", \"water\", \"oil\", \"soap\", \"sugar\", \"salt\", \"honey\"\n",
    "]\n",
    "\n",
    "class ValidatorModel:\n",
    "    def __init__(self,words = game_words):\n",
    "        self.word_list = words\n",
    "        self.keyword = words[0]\n",
    "        self.turn = 0\n",
    "        self.guess_number = 0\n",
    "        \n",
    "    def validate_question(self,question):        \n",
    "        system_prompt = (f\"Let's play 20 Questions. You are playing the role of the Answerer.The keyword is **{self.keyword}**.\")\n",
    "        \n",
    "        user_prompt = (\n",
    "            f'''The question is about the keyword **{self.keyword}** .\n",
    "            Give yes-or-no answer about the keyword and surround your answer with just one word, like yes or no.\n",
    "            The Question is **{question}**'''\n",
    "        )\n",
    "        \n",
    "        prompt = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                  {\"role\": \"user\",   \"content\": user_prompt}]\n",
    "        model_ans = Ask_AI(prompt, max_new=4, temp=0).strip().split()[0]\n",
    "        return model_ans\n",
    "\n",
    "    def validate_guess(self,guess):\n",
    "        self.guess_number += 1\n",
    "        out = 'Yes' if guess.lower() == self.keyword.lower() else 'No'\n",
    "        if out.lower() == 'yes' or self.guess_number == 20:\n",
    "            self.turn += 1\n",
    "            if self.turn < len(self.word_list):\n",
    "                self.keyword = (self.word_list)[self.turn]\n",
    "            self.guess_number = 0\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "196f9adb-3945-450b-b737-90a440a84a86",
    "_uuid": "d226ea34-7ba6-47fe-9820-01e67d82a4e3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %load validator_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code reads ValidatorModel class from the file and play N 20-Question games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e9f02a86-1efe-4e81-86da-f055838bbf40",
    "_uuid": "887f9e82-18d9-4b58-9e87-82e685c28597",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from validator_model import ValidatorModel\n",
    "\n",
    "validator = ValidatorModel()\n",
    "\n",
    "bot = GameWebRAGMemoryChatbot(\n",
    "        exa_api_key=os.getenv(\"EXA_API_KEY\"),\n",
    "        retriever=None,\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        rt = 0\n",
    ")\n",
    "\n",
    "q_all = []\n",
    "win = 0\n",
    "print_flag = False #True\n",
    "\n",
    "Number_of_games = input(\"How many games do you want?(<100)\")\n",
    "Number_of_games = int(Number_of_games)\n",
    "Number_of_games = Number_of_games if Number_of_games <= 100 else 100\n",
    "True_guess = []\n",
    "print(f\"Playing {Number_of_games} games: \")\n",
    "for _ in range(Number_of_games):\n",
    "    # print(\"New keyword is: \",validator.keyword)\n",
    "    start_game_prompt = \"I wanna play 20 question.\"\n",
    "    model_ques = bot.ask(start_game_prompt)\n",
    "    querries = []\n",
    "    while True:\n",
    "        if print_flag:\n",
    "            print(\"Model Question: \",model_ques)\n",
    "            \n",
    "        validator_question_ans = validator.validate_question(model_ques)\n",
    "        if print_flag:\n",
    "            print(\"Validator ans: \",validator_question_ans)\n",
    "            \n",
    "        model_guess = bot.ask(validator_question_ans)\n",
    "        if print_flag:\n",
    "            print(\"Model guess: \",model_guess)\n",
    "\n",
    "        validator_guess_ans =  validator.validate_guess(model_guess)\n",
    "        if print_flag:\n",
    "            print(\"Validator ans to guess: \",validator_guess_ans)\n",
    "            \n",
    "        querries.append({\"Model Question\":model_ques,\"validator answer\":validator_question_ans\n",
    "                        ,\"Model guess\":model_guess ,\"Validator answer_g\":validator_guess_ans })\n",
    "        \n",
    "        # New question or End of the game\n",
    "        model_ques = bot.ask(validator_guess_ans)\n",
    "        \n",
    "        if model_ques == \"üéâ I guessed it! Thanks for playing.\" or model_ques == \"üòî I couldn't get it in 20 tries. You win!\":\n",
    "            if model_ques == \"üéâ I guessed it! Thanks for playing.\":\n",
    "                win += 1\n",
    "                True_guess.append(model_guess)\n",
    "            print(\"End of the game:\")\n",
    "            print(model_ques)\n",
    "            break\n",
    "    q_all.append(querries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T10:18:36.543634Z",
     "iopub.status.busy": "2025-08-18T10:18:36.542926Z",
     "iopub.status.idle": "2025-08-18T10:18:36.547976Z",
     "shell.execute_reply": "2025-08-18T10:18:36.547174Z",
     "shell.execute_reply.started": "2025-08-18T10:18:36.543608Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 games won out of 20 games.\n",
      "True guesses:  ['cat', 'dog', 'cow', 'horse', 'lion', 'bear', 'shark', 'cheese', 'pizza', 'egg']\n"
     ]
    }
   ],
   "source": [
    "print(f\"{win} games won out of {Number_of_games} games.\")\n",
    "print(\"True guesses: \",True_guess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "3603920b-23d5-4df9-87e6-a2b07294c18a",
    "_uuid": "6f6d6157-6b09-419e-ac45-eef94abaf1da",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-17T16:45:09.301520Z",
     "iopub.status.busy": "2025-08-17T16:45:09.301217Z",
     "iopub.status.idle": "2025-08-17T16:45:09.306046Z",
     "shell.execute_reply": "2025-08-17T16:45:09.305279Z",
     "shell.execute_reply.started": "2025-08-17T16:45:09.301499Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 games won out of 100 games.\n",
      "True guesses:  ['cat', 'dog', 'cow', 'horse', 'lion', 'bear', 'shark', 'bread', 'cheese', 'pizza', 'egg', 'chair', 'car', 'bicycle', 'bus', 'train', 'airplane', 'boat', 'scissors', 'eraser', 'laptop', 'drum', 'guitar', 'camera', 'radio', 'television', 'jacket', 'hat', 'river', 'ocean', 'doctor', 'singer', 'gold', 'silver', 'iron', 'water']\n"
     ]
    }
   ],
   "source": [
    "# Statistic and results of the games.\n",
    "\n",
    "print(f\"{win} games won out of {Number_of_games} games.\")\n",
    "print(\"True guesses: \",True_guess)\n",
    "\n",
    "# for qu in q_all:\n",
    "#     for q in qu:\n",
    "#         print(\"Model question: \", q['Model Question'])\n",
    "#         print(\"Validator answer: \", q['validator answer'])\n",
    "#         print(\"Model guess: \",q['Model guess'])\n",
    "#         print(\"Validator answer: \",q['Validator answer_g'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fdea3c6a-ffa7-4dc8-95bc-568fc9edf0aa",
    "_uuid": "daf683bb-a68d-405f-bedf-0ceb89627a93",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# Part 4: UI interface **<span style=\"color:blue;\">R**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "d90a4873-925b-4b9a-847a-edc29846e4e2",
    "_uuid": "3033a47d-eb64-470f-99e1-3794a330c193",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-18T10:19:11.831062Z",
     "iopub.status.busy": "2025-08-18T10:19:11.830789Z",
     "iopub.status.idle": "2025-08-18T10:19:19.719556Z",
     "shell.execute_reply": "2025-08-18T10:19:19.719060Z",
     "shell.execute_reply.started": "2025-08-18T10:19:11.831040Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e76073bde14e48bb6292b6781ef9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f67ccd5287c244b681835f11daf4bcae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e62c8f4e86e4886a9ca25c19d672865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e0fea098a24cf3ba16c124e7d670de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff228d7153564b1b8f3930db3aa94a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152a33cefa8a4e238249073004a2cc91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ed632a6da344fa84050aa9924910f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f529d63e835485cb48de75e7e86875c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b7f9772afc4e92876b7fe73597e392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365b13feec5448079fd9b61c67c6be96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655155e5c8bb464ba9ccbdbdefc3eea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "* Running on public URL: https://32af2ffaa1278b4b64.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://32af2ffaa1278b4b64.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web search done!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web search done!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web search done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Settings for reading file during conversation:\n",
    "\n",
    "CHUNK_SIZE   = 700                                    # characters per chunk\n",
    "TOP_K        = 3                                      # retrieve this many chunks\n",
    "current_pdf_name = None      # just for the UI title/description\n",
    "embedder   = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "bot = GameWebRAGMemoryChatbot(\n",
    "    exa_api_key=os.getenv(\"EXA_API_KEY\"),\n",
    "    retriever=None,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    rt = 1\n",
    ")\n",
    "\n",
    "class SimpleRetriever:\n",
    "    \"\"\"Holds the FAISS index + metadata for one document.\"\"\"\n",
    "    def __init__(self, vecs: np.ndarray, chunks: list[str]):\n",
    "        self.index = faiss.IndexFlatL2(vecs.shape[1])\n",
    "        self.index.add(vecs)\n",
    "        self.chunks = chunks\n",
    "        # all vectors encoded by MiniLM have the same length; save norm once\n",
    "        self._norm = np.linalg.norm(vecs[0])\n",
    "\n",
    "    def _sim(self, l2: float) -> float:          # convert L2 ‚Üí cosine-like score\n",
    "        return max(0.0, 1.0 - l2 / (2 * self._norm ** 2))\n",
    "\n",
    "    def top_k(self, query: str, k: int = TOP_K):\n",
    "        q = embedder.encode([query]).astype(\"float32\")\n",
    "        D, I = self.index.search(q, k)\n",
    "        hits = []\n",
    "        for rank, (idx, dist) in enumerate(zip(I[0], D[0])):\n",
    "            if idx == -1:                 # should not happen here\n",
    "                continue\n",
    "            hits.append({\n",
    "                \"rank\": rank + 1,\n",
    "                \"sim\" : round(self._sim(dist), 3),\n",
    "                \"text\": self.chunks[idx]\n",
    "            })\n",
    "        return hits\n",
    "\n",
    "def load_pdf(fileobj) -> str:\n",
    "    \"\"\"Extract all text from an uploaded PDF file-like object.\"\"\"\n",
    "    with fitz.open(fileobj.name) as doc:\n",
    "        return \"\\n\".join(page.get_text() for page in doc)\n",
    "        \n",
    "def clean(text: str) -> str:\n",
    "    \"\"\"Minimal cleanup ‚Äì collapse whitespace, keep punctuation.\"\"\"\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def chunk_text(text: str, size: int = CHUNK_SIZE):\n",
    "    for start in range(0, len(text), size):\n",
    "        yield text[start : start + size]\n",
    "\n",
    "# ‚îÄ‚îÄ Define the reply function Gradio will call ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def respond(message: str, history: list[tuple[str, str]]):\n",
    "    # print(history)\n",
    "    assistant_text = bot.ask(message)\n",
    "    return assistant_text\n",
    "\n",
    "def build_retriever(pdf_file):\n",
    "    \"\"\"Create retriever from the uploaded PDF, store it in global state.\"\"\"\n",
    "    global current_pdf_name\n",
    "    text = clean(load_pdf(pdf_file))\n",
    "    \n",
    "    chunks = list(chunk_text(text))\n",
    "    vecs  = embedder.encode(chunks, batch_size=32, show_progress_bar=False).astype(\"float32\")\n",
    "    \n",
    "    bot.retriever = SimpleRetriever(vecs, chunks)\n",
    "    current_pdf_name = os.path.basename(pdf_file.name)\n",
    "    \n",
    "    return f\"‚úÖ Loaded **{current_pdf_name}** with {len(chunks)} chunks.\" \n",
    "\n",
    "\n",
    "# 2. ‚îÄ‚îÄ Spin up the Chat UI\n",
    " \n",
    "with gr.Blocks(title=\"LLM Chat\") as demo:\n",
    "    gr.Markdown(\"## ü§ñ  My Local LLM Chat\\n\"\n",
    "                \"1. You can upload **one** PDF via the file box and ask about it.\\n\"\n",
    "                \"2. Ask questions ‚Äì the chat will automatically consult the file.\\n\"\n",
    "                \"3. You can also ask about web info.\\n\"\n",
    "                 \"4. You can play 20-question game.\")\n",
    "    upload_box = gr.File(label=\"üìé  Upload a PDF\", file_types=[\".pdf\"], file_count=\"single\")\n",
    "    status_box = gr.Markdown()\n",
    "    chat = gr.ChatInterface(\n",
    "                        fn=respond,\n",
    "                        description=\"Ask me anything ‚Äì I‚Äôm running on your GPU/CPU.\",\n",
    "                        examples=[\"Hello!\", \"What is latest news about Bitcoin?\", \"let's play 20-Question!\"]\n",
    "                        )\n",
    "    upload_box.upload(build_retriever, upload_box, status_box)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code write a file named evaluate_20Q.py for evaluating 20-Question game. First run below cell and then for running evaluate_20Q.py use this script in console in Kaggle (N is number of games):\n",
    "\n",
    "<< !python evaluate_20Q.py -N 100 >>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T11:46:56.447677Z",
     "iopub.status.busy": "2025-08-18T11:46:56.446964Z",
     "iopub.status.idle": "2025-08-18T11:46:56.464048Z",
     "shell.execute_reply": "2025-08-18T11:46:56.463327Z",
     "shell.execute_reply.started": "2025-08-18T11:46:56.447645Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting evaluate_20Q.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile evaluate_20Q.py\n",
    "\n",
    "# # Installing modules\n",
    "import subprocess, sys, importlib\n",
    "\n",
    "\n",
    "def ensure_package(pkg, version=None):\n",
    "    import importlib, subprocess, sys\n",
    "    try:\n",
    "        if version:\n",
    "            importlib.import_module(pkg)\n",
    "            return\n",
    "    except ImportError:\n",
    "        pass\n",
    "    target = f\"{pkg}=={version}\" if version else pkg\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", target])\n",
    "\n",
    "\n",
    "ensure_package(\"transformers\")\n",
    "ensure_package(\"accelerate\")\n",
    "# ensure_package(\"bitsandbytes\")\n",
    "ensure_package(\"bitsandbytes\", \"0.46.0\")          # CUDA 11.8 build\n",
    "ensure_package(\"sentence-transformers\")\n",
    "ensure_package(\"faiss-gpu-cu12\")\n",
    "ensure_package(\"pymupdf>=1.22\")\n",
    "ensure_package(\"exa-py\")\n",
    "ensure_package(\"python-dotenv\")\n",
    "\n",
    "# Importing modules\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch, gc, os\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re \n",
    "import glob, json, itertools, math, pathlib\n",
    "import fitz                           # PyMuPDF\n",
    "import time\n",
    "from collections import deque\n",
    "from typing import List, Dict\n",
    "import requests, textwrap\n",
    "from exa_py import Exa\n",
    "import textwrap, random\n",
    "\n",
    "# reading ValidatorModel class from the file\n",
    "\n",
    "# from validator_model  import ValidatorModel\n",
    "from test  import ValidatorModel\n",
    "\n",
    "# Reading script\n",
    "if \"-N\" in sys.argv:\n",
    "    n_index = sys.argv.index(\"-N\") + 1\n",
    "    Number_of_games = int(sys.argv[n_index])\n",
    "else:\n",
    "    raise ValueError(\"Use -N <number>\")\n",
    "    \n",
    "print(\"Running\", Number_of_games, \"rounds\")\n",
    "\n",
    "# hugging face token\n",
    "# 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\n",
    "\n",
    "# # Section 1 : Chat bot core\n",
    "\n",
    "os.environ[\"EXA_API_KEY\"] = \"af19a97b-45de-4ab4-8344-7029c5b7e7d6\"\n",
    "EXA_ENDPOINT = \"https://api.exa.ai/search\"  \n",
    "\n",
    "login(\"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\")\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "bnb_cfg = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                             bnb_4bit_compute_dtype=torch.float16,\n",
    "                             bnb_4bit_use_double_quant=True,\n",
    "                             bnb_4bit_quant_type=\"nf4\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_cfg,\n",
    "    trust_remote_code=True,\n",
    ").eval()\n",
    "\n",
    "# ------------ tweakables -----------------------------------------------------\n",
    "SYSTEM_PROMPT = \"You are a helpful AI assistant.\"\n",
    "MAX_CTX_TOKENS   = 8000 - 512        # keep 512 tokens headroom\n",
    "SUMMARISE_AT_TOK = 6000              # start summarising above this\n",
    "CHUNK_SIZE       = 12               # summarise 12 oldest turns each time\n",
    "LOG_FILE         = \"chatlog.jsonl\"   # optional disk log\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def num_tokens(text: str) -> int:\n",
    "    # helper for quick token counting\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def chat_completion(messages: List[Dict],  # messages[-1] must be user\n",
    "                    max_new=256, temp=0.7, top_p=0.9):\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new,\n",
    "        temperature=temp,\n",
    "        top_p=top_p,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    reply = tokenizer.decode(out[0][inputs.input_ids.shape[1]:],\n",
    "                             skip_special_tokens=True).strip()\n",
    "    return reply\n",
    "\n",
    "\n",
    "class MemoryChatbot:\n",
    "    \"\"\"Keeps the last N turns verbatim and auto-summarises earlier ones.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 system_prompt: str = SYSTEM_PROMPT,\n",
    "                 max_ctx_tokens: int = MAX_CTX_TOKENS,\n",
    "                 summarise_at: int = SUMMARISE_AT_TOK,\n",
    "                 chunk_size: int = CHUNK_SIZE):\n",
    "        self.system_prompt = system_prompt\n",
    "        self.max_ctx_tokens = max_ctx_tokens\n",
    "        self.summarise_at   = summarise_at\n",
    "        self.chunk_size     = chunk_size\n",
    "\n",
    "        self.history = deque()    # list of {\"role\":..., \"content\":...}\n",
    "        self.memo    = \"\"         # running summary of trimmed turns\n",
    "\n",
    "    # ------------- public API -------------------------------------------------\n",
    "    def ask(self, user_msg: str) -> str:\n",
    "        \"\"\"Main entry: add user message ‚Üí maybe summarise ‚Üí get reply.\"\"\"\n",
    "        self._append(\"user\", user_msg)\n",
    "        self._maybe_summarise()\n",
    "        reply = self._generate_reply(user_msg)\n",
    "        self._append(\"assistant\", reply)\n",
    "        return reply\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ------------- internal helpers ------------------------------------------\n",
    "    def _append(self, role, content):\n",
    "        self.history.append({\"role\": role, \"content\": content})\n",
    "        self._disk_log(role, content)\n",
    "\n",
    "    def _current_messages(self) -> List[Dict]:\n",
    "        sys = self.system_prompt\n",
    "        if self.memo:\n",
    "            sys = f\"{sys}\\n\\n[CONTEXT SUMMARY]\\n{self.memo}\"\n",
    "        msgs = [{\"role\": \"system\", \"content\": sys}]\n",
    "        msgs.extend(self.history)  # history starts with 'user'\n",
    "        return msgs\n",
    "        \n",
    "    def _prompt_tokens(self) -> int:\n",
    "        txt = tokenizer.apply_chat_template(self._current_messages(),\n",
    "                                            tokenize=False)\n",
    "        return num_tokens(txt)\n",
    "\n",
    "    def _maybe_summarise(self):\n",
    "        \"\"\"If conversation is getting heavy, summarise oldest chunk.\"\"\"\n",
    "        while self._prompt_tokens() > self.summarise_at and len(self.history) > self.chunk_size:\n",
    "            print(\"<<<Summarizing>>>\")\n",
    "            chunk = list(self.history)[:self.chunk_size]\n",
    "            summary = []\n",
    "            for i in range(0,len(chunk),2):\n",
    "                chunk_txt = f\"{chunk[i]['role']}: {chunk[i]['content']} \\n {chunk[i+1]['role']}: {chunk[i+1]['content']}\"\n",
    "                summary_prompt = [\n",
    "                    {\"role\": \"system\",\n",
    "                     \"content\": \"You are a summarisation assistant.\"},\n",
    "                    {\"role\": \"user\",\n",
    "                     \"content\":\n",
    "                     (\"Summarise the following conversation in one sentence, \"\n",
    "                      \"preserve all factual details:\\n\\n\" + chunk_txt)}\n",
    "                ]\n",
    "                summary.append(chat_completion(summary_prompt, max_new=30, temp=0.1))\n",
    "            \n",
    "            last_summary = \"\\n \".join(s for s in summary)\n",
    "            # remove chunk & prepend summary\n",
    "            for _ in range(self.chunk_size):\n",
    "                self.history.popleft()\n",
    "            if len(self.memo.split(\"\\n\")) > 40:\n",
    "                old_memo = self.memo.split(\"\\n\")\n",
    "                old_memo = \"\\n \".join(s for s in old_memo[15:])\n",
    "                self.memo = (old_memo + \"\\n\" + last_summary).strip()\n",
    "            else:\n",
    "                self.memo = (self.memo + \"\\n\" + last_summary).strip()\n",
    "            # free GPU RAM\n",
    "            gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "            if self._prompt_tokens() < self.max_ctx_tokens:\n",
    "                break\n",
    "\n",
    "    def _generate_reply(self, user_msg):\n",
    "        msgs = self._current_messages()\n",
    "        reply = chat_completion(msgs)\n",
    "        gc.collect(); torch.cuda.empty_cache()\n",
    "        return reply\n",
    "\n",
    "    def _disk_log(self, role, content):\n",
    "        if LOG_FILE:\n",
    "            with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "                json.dump({\"ts\": time.time(), \"role\": role,\n",
    "                           \"content\": content}, f, ensure_ascii=False)\n",
    "                f.write(\"\\n\")\n",
    "    # -------------------------------------------------------------------------        \n",
    "\n",
    "# # Section 3 : Function call\n",
    "\n",
    "# Part 3.1\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Section 3 ‚Äì Web-augmented RAG + Memory\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "class WebRAGMemoryChatbot(MemoryChatbot):\n",
    "    \"\"\"\n",
    "    RAG + Memory + web trigger (Part 3.1)\n",
    "    --------------------------------------------------------------------------\n",
    "    ‚Ä¢ inherits history, summarisation, and local-PDF RAG from Section 2\n",
    "    ‚Ä¢ adds:\n",
    "        ‚Äì _needs_web_search   ‚Üí yes/no (with LLM)\n",
    "        ‚Äì _exa_search         ‚Üí 3 hits  (Exa API)\n",
    "        ‚Äì _summarise_hits     ‚Üí ‚â§120-word digest\n",
    "    \"\"\"\n",
    "    # ---------- init ---------------------------------------------------------\n",
    "    def __init__(self, retriever , exa_api_key: str, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)         # from RAGMemoryChatbot\n",
    "        self.exa_api_key = exa_api_key\n",
    "        self.exa = Exa(exa_api_key)   \n",
    "        self.retriever = retriever\n",
    "    # ---------- helpers ------------------------------------------------------        \n",
    "    def _needs_web_search(self, user_msg: str) -> bool:\n",
    "        \"\"\"\n",
    "        Upgraded classifier to determine if a web search is necessary.\n",
    "        \"\"\"\n",
    "        \n",
    "        # The new, more descriptive prompt with criteria and examples\n",
    "        prompt = f'''\n",
    "        You are an expert classifier. Your task is to determine if a user's question requires a real-time web search.\n",
    "        Reply with a single word: YES or NO.\n",
    "        \n",
    "        ## CRITERIA\n",
    "        You must answer YES if the question asks for:\n",
    "        - Current events or news (e.g., \"what happened in France today?\")\n",
    "        - Real-time information (e.g., \"what's the price of gold?\", \"what's the weather in London?\")\n",
    "        - Information about a very recent topic or public figure.\n",
    "        \n",
    "        You must answer NO if the question is about:\n",
    "        - General knowledge (e.g., \"what is the capital of Japan?\")\n",
    "        - Math, logic, or creative writing.\n",
    "        - Information contained within provided documents.\n",
    "        - A greeting or a question about your identity.\n",
    "        \n",
    "        ## EXAMPLES\n",
    "        User: What is the weather like in Tehran today?\n",
    "        Assistant: YES\n",
    "        \n",
    "        User: Can you tell me a story about a dragon?\n",
    "        Assistant: NO\n",
    "        \n",
    "        User: what news are about Syria today?\n",
    "        Assistant: YES\n",
    "        \n",
    "        User: what is information about persian cat in doc?\n",
    "        Assistant: NO\n",
    "        \n",
    "        ## TASK\n",
    "        Now, classify the following user question. Remember to only reply with YES or NO.\n",
    "        \n",
    "        User: {user_msg}\n",
    "        Assistant:\n",
    "        '''\n",
    "    \n",
    "        messages = [\n",
    "            # The detailed instructions are now in the user message for better focus\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        # Increase max_new_tokens to safely generate \"yes\" or \"no\"\n",
    "        # Lower temperature for more stable, deterministic classification\n",
    "        out = chat_completion(messages, max_new=4, temp=0.1).strip().lower()\n",
    "        # Check for a clean \"yes\"\n",
    "        return out.startswith(\"yes\")\n",
    "\n",
    "    def _exa_search(self, query: str, k: int = 3):\n",
    "        \"\"\"\n",
    "        Uses exa_py.search_and_contents ‚Üí one call does both search + content\n",
    "        Returns a list[{title,url,summary}] for downstream summarisation.\n",
    "        \"\"\"\n",
    "        if not self.exa_api_key:\n",
    "            raise ValueError(\"‚ùå EXA_API_KEY is empty or not set.\")\n",
    "    \n",
    "        # call the SDK ‚Äì we want full page text, not just metadata\n",
    "        response = self.exa.search_and_contents(\n",
    "            query,\n",
    "            text=True,                # full text of each result\n",
    "            num_results=k,             # ‚Üê snake_case in SDK\n",
    "            # contex = True,\n",
    "            summary = True\n",
    "        )                              # :contentReference[oaicite:0]{index=0}\n",
    "        hits = []\n",
    "        for r in response.results:     # ResultWithText objects\n",
    "            hits.append({\n",
    "                \"title\":   r.title,\n",
    "                \"url\":     r.url,\n",
    "                \"text\": (r.text or \"\")#[:600]      # first 400 chars\n",
    "                ,\"summary\": r.summary\n",
    "            })\n",
    "        return hits\n",
    "\n",
    "\n",
    "    def _summarise_hits(self, hits) -> str:\n",
    "        joined = \"\\n\".join(f\"‚Ä¢ {h['title']}: {h['summary']}\" for h in hits)\n",
    "        prompt = [\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": \"Summarise the following web snippets in ‚â§120 words.\"},\n",
    "            {\"role\": \"user\", \"content\": joined}\n",
    "        ]\n",
    "        return chat_completion(prompt, max_new=160, temp=0.3).strip()\n",
    "\n",
    "\n",
    "    # ---------- public API ---------------------------------------------------\n",
    "    def ask(self, user_msg: str, k: int = 3) -> str:\n",
    "        # 1) bookkeeping + maybe summarise\n",
    "        self._append(\"user\", user_msg)\n",
    "        self._maybe_summarise()\n",
    "        # 2) RAG (PDF)\n",
    "        rag_block = \"\"\n",
    "        if self.retriever is not None:\n",
    "            hits = self.retriever.top_k(user_msg, k=k)\n",
    "            if hits:\n",
    "                rag_block = (\"\\n\\n---\\n[Local Doc]\\n Relevant background:\\n\" +\n",
    "                             \"  \".join(f\" {h['text']}\" for h in hits) +\n",
    "                             \"\\n---\")\n",
    "\n",
    "        # 3) Web-Trigger\n",
    "        web_block = \"\"\n",
    "        summaries = \"\"\n",
    "        if self._needs_web_search(user_msg):\n",
    "            try:\n",
    "                web_hits = self._exa_search(user_msg, k=3)\n",
    "                summary  = self._summarise_hits(web_hits)\n",
    "                web_block = f\"\\n\\n---\\n[Web Info]\\n{summary}\\n---\"\n",
    "                print(\"Web search done!\\n\")\n",
    "\n",
    "                for h in web_hits:\n",
    "                    summaries+= h[\"summary\"] \n",
    "            except Exception as e:\n",
    "                print(\"‚ö†Ô∏è  Web search failed:\", e)\n",
    "        \n",
    "\n",
    "        # 4) build full prompt & get answer\n",
    "        messages = self._current_messages()        # system + history (+memo)\n",
    "        messages[0][\"content\"] +=  rag_block + web_block \n",
    "        reply = chat_completion(messages,max_new=512, temp=0.5)\n",
    "\n",
    "        # 5) save reply\n",
    "        self._append(\"assistant\", reply)\n",
    "        return reply\n",
    "\n",
    "\n",
    "# Part 3.2\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Game-aware Web-RAG + Memory Chatbot  (20-Questions, EN-only)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "class GameWebRAGMemoryChatbot(WebRAGMemoryChatbot):\n",
    "    # --------------------------------------------------------------------- #\n",
    "    # 0. constructor & single-point game-state init\n",
    "    # --------------------------------------------------------------------- #\n",
    "    def __init__(self, retriever , exa_api_key: str, *args, **kwargs):\n",
    "        super().__init__(retriever,exa_api_key, *args, **kwargs)\n",
    "        self._init_game_state()\n",
    "\n",
    "    def _init_game_state(self):\n",
    "        self.game_active     = False\n",
    "        self.phase           = \"ask\"     # \"ask\" | \"guess\"\n",
    "        self.questions_asked = 0\n",
    "        self.max_questions   = 20\n",
    "        self.history_qna     = []        # [(question, answer), ‚Ä¶]\n",
    "        self._asked_set      = set()     # lower-cased questions\n",
    "        self._guess_set      = set()     # lower-cased guesses\n",
    "    # --------------------------------------------------------------------- #\n",
    "    # 1. intent detection helpers\n",
    "    # --------------------------------------------------------------------- #\n",
    "    def _detect_game_start(self, user_msg: str) -> bool:\n",
    "        prompt = [\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": (\"You are a classifier. Respond with *exactly* YES or NO \"\n",
    "                         \"(uppercase, no punctuation) to the following question:\\n\"\n",
    "                         \"Does the user explicitly ask to PLAY a 20-Questions game?\\n\"\n",
    "                         \"If they only greet, ask who you are, or anything else, answer NO.\")},\n",
    "            {\"role\": \"user\", \"content\": user_msg.strip()}\n",
    "        ]\n",
    "        reply = chat_completion(prompt,\n",
    "                            max_new=1,\n",
    "                            temp=0.0,\n",
    "                            top_p=0.0)\n",
    "        \n",
    "        return reply.strip().lower().startswith(\"y\")\n",
    "\n",
    "    def _detect_quit(self, user_msg: str) -> bool:\n",
    "        return bool(re.search(r\"\\b(quit|exit|cancel|stop|end|end game)\\b\",\n",
    "                              user_msg, flags=re.I))\n",
    "        \n",
    "    # --------------------------------------------------------------------- #\n",
    "    # 2. guess and question generator \n",
    "    # --------------------------------------------------------------------- #\n",
    "    \n",
    "    def _generate_question(self) -> str:\n",
    "       # ------------------------------------------------------------------ #\n",
    "        # 1) dynamic LLM-generated question\n",
    "        # ------------------------------------------------------------------ #\n",
    "        history_summary = \"\\\\n\".join([f\"Q: {q} | A: {a}\" for q, a in self.history_qna])\n",
    "        asked_questions = \"\\\\n\".join(sorted(self._asked_set))\n",
    "    \n",
    "        system_prompt = (\n",
    "            \"You are playing a game of 20 Questions. Your task is to ask a strategic and simple 'yes' or 'no' question to help you guess the secret object. The question must be new and not a repeat of one you have asked before.\"\n",
    "        )\n",
    "        user_prompt = (\n",
    "            f\"Here is the conversation history so far:\\\\n{history_summary}\\\\n\\\\n\"\n",
    "            f\"Here are the questions you have already asked:\\\\n{asked_questions}\\\\n\\\\n\"\n",
    "            \"Based on the history, generate the next logical and simple 'yes' or 'no' question to ask. The question should be concise and clear (under 12 words).\\\\n\\\\n\"\n",
    "            \"**IMPORTANT RULES:**\\\\n\"\n",
    "            \"- The question MUST be answerable with a simple 'yes' or 'no'.\\\\n\"\n",
    "            \"- DO NOT ask questions that contain the word 'or' (e.g., 'Is it a liquid or a gas?').\\\\n\"\n",
    "            \"- DO NOT ask for a guess.\\\\n\\\\n\"\n",
    "            \"Generate only the question.\"\n",
    "        )\n",
    "    \n",
    "        prompt = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                  {\"role\": \"user\",   \"content\": user_prompt}]\n",
    "    \n",
    "        candidate = chat_completion(prompt, max_new=20, temp=0.4).strip()\n",
    "        # Clean up the generated question\n",
    "        candidate = candidate.split(\"?\")[0].strip().capitalize() + \"?\"\n",
    "        if (2 <= len(candidate.split()) <= 20\n",
    "                and candidate.lower() not in self._asked_set):\n",
    "            self._asked_set.add(candidate.lower())\n",
    "            return candidate\n",
    "    \n",
    "        # ------------------------------------------------------------------ #\n",
    "        # 2) ultimate static fall-back\n",
    "        # ------------------------------------------------------------------ #\n",
    "        FALLBACKS = [\n",
    "            \"Is it man-made?\",\n",
    "            \"Is it bigger than a loaf of bread?\",\n",
    "            \"Is it commonly found indoors?\",\n",
    "            \"Is it electronic?\",\n",
    "        ]\n",
    "        for fb in FALLBACKS:\n",
    "            fb_l = fb.lower()\n",
    "            if fb_l not in self._asked_set:\n",
    "                self._asked_set.add(fb_l)\n",
    "                return fb\n",
    "    \n",
    "        # If absolutely everything else fails\n",
    "        return \"Is it tangible?\"\n",
    "\n",
    "    def _generate_guess(self) -> str:\n",
    "    \n",
    "        history_summary = \"\\\\n\".join([f\"Q: {q} | A: {a}\" for q, a in self.history_qna])\n",
    "        forbidden_guesses = \", \".join(self._guess_set)\n",
    "        \n",
    "        system_prompt = (\n",
    "            \"You are playing a game of 20 Questions. Your task is to guess the secret item. Based on the provided history of questions and answers, your goal is to provide a single, concrete noun as your guess. Do not provide any explanation or surrounding text.\"\n",
    "        )\n",
    "        user_prompt = (\n",
    "            f\"Here is the history of the game so far:\\\\n{history_summary}\\\\n\\\\n\"\n",
    "            f\"You have already guessed the following words: {forbidden_guesses}\\\\n\\\\n\"\n",
    "            \"Based on this information, what is your single-word guess for the secret item? Your response must be a single word and a concrete noun.\"\n",
    "        )\n",
    "        prompt = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                  {\"role\": \"user\",   \"content\": user_prompt or \"[start]\"}]\n",
    "    \n",
    "        g = chat_completion(prompt, max_new=4, temp=0.0).strip()\n",
    "        # Take the first word if multiple are generated\n",
    "        g = g.split()[0]\n",
    "        g = re.sub(r\"[^a-zA-Z]\", \"\", g).lower()\n",
    "        BAD_GUESSES = {\"\", \"thing\", \"object\", \"based\",\"i\",\"my\"}\n",
    "        if g and len(g) > 1 and g not in self._guess_set and g not in BAD_GUESSES:\n",
    "            return g\n",
    "            \n",
    "        GUESS_FALLBACKS = [\"cat\", \"dog\", \"tree\", \"car\", \"phone\", \"apple\", \"lion\"]\n",
    "\n",
    "        for fb in GUESS_FALLBACKS:\n",
    "            if fb not in self._guess_set:\n",
    "                return fb\n",
    "        return \"idea\"\n",
    "    # --------------------------------------------------------------------- #\n",
    "    # 3. public ask()  (state machine)\n",
    "    # --------------------------------------------------------------------- #\n",
    "    def ask(self, user_msg: str, k: int = 3) -> str:\n",
    "        # -------- escape hatch --------\n",
    "        if self.game_active and self._detect_quit(user_msg):\n",
    "            self._init_game_state()\n",
    "            return \"Game stopped. Say 'play 20 questions' to start again.\"\n",
    "        # -------- active game ---------\n",
    "        if self.game_active:\n",
    "            # user answered a QUESTION  ‚Üí now make a GUESS\n",
    "            if self.phase == \"ask\":\n",
    "                self.history_qna[-1] = (self.history_qna[-1][0], user_msg.strip())\n",
    "                guess = self._generate_guess()\n",
    "                self._guess_set.add(guess.lower())\n",
    "                self.phase = \"guess\"\n",
    "                # return f\"Is it **{guess}**? (Yes/No)\"\n",
    "                return guess.lower()\n",
    "            # user judged our GUESS\n",
    "            if self.phase == \"guess\":\n",
    "                if user_msg.lower().startswith(\"yes\"):\n",
    "                    self._init_game_state()\n",
    "                    return \"I guessed it! Thanks for playing.\"\n",
    "                # wrong guess\n",
    "                self.questions_asked += 1\n",
    "                if self.questions_asked >= self.max_questions:\n",
    "                    self._init_game_state()\n",
    "                    return \"I couldn't get it in 20 tries. You win!\"\n",
    "                # next question\n",
    "                q = self._generate_question()\n",
    "                self.history_qna.append((q, None))\n",
    "                self._asked_set.add(q.lower())\n",
    "                self.phase = \"ask\"\n",
    "                return q + \" (Yes/No)\"\n",
    "        # -------- start trigger -------\n",
    "        if self._detect_game_start(user_msg):\n",
    "            self._init_game_state()\n",
    "            self.game_active = True\n",
    "            first_q = self._generate_question()\n",
    "            self.history_qna.append((first_q, None))\n",
    "            self._asked_set.add(first_q.lower())\n",
    "            intro = (\"Let's play 20 Questions! Think of a word; \"\n",
    "                     \"I'll guess in ‚â§20 yes/no questions.\")\n",
    "            return first_q + \" (Yes/No)\"\n",
    "        # -------- normal chat ---------\n",
    "        return super().ask(user_msg, k=k)\n",
    "\n",
    "\n",
    "# %load validator_model.py\n",
    "\n",
    "validator = ValidatorModel()\n",
    "\n",
    "bot = GameWebRAGMemoryChatbot(\n",
    "        exa_api_key=os.getenv(\"EXA_API_KEY\"),\n",
    "        retriever=None,\n",
    "        system_prompt=SYSTEM_PROMPT\n",
    ")\n",
    "\n",
    "q_all = []\n",
    "win = 0\n",
    "print_flag = False #True\n",
    "Number_of_games = int(Number_of_games)\n",
    "True_guess = []\n",
    "print(f\"Playing {Number_of_games} games: \")\n",
    "for _ in range(Number_of_games):\n",
    "    start_game_prompt = \"I wanna play 20 question.\"\n",
    "    model_ques = bot.ask(start_game_prompt)\n",
    "    querries = []\n",
    "    while True:\n",
    "        if print_flag:\n",
    "            print(\"Model Question: \",model_ques)\n",
    "            \n",
    "        validator_question_ans = validator.validate_question(model_ques)\n",
    "        if print_flag:\n",
    "            print(\"Validator ans: \",validator_question_ans)\n",
    "            \n",
    "        model_guess = bot.ask(validator_question_ans)\n",
    "        if print_flag:\n",
    "            print(\"Model guess: \",model_guess)\n",
    "\n",
    "        validator_guess_ans =  validator.validate_guess(model_guess)\n",
    "        if print_flag:\n",
    "            print(\"Validator ans to guess: \",validator_guess_ans)\n",
    "            \n",
    "        querries.append({\"Model Question\":model_ques,\"validator answer\":validator_question_ans\n",
    "                        ,\"Model guess\":model_guess ,\"Validator answer_g\":validator_guess_ans })\n",
    "        \n",
    "        # New question or End of the game\n",
    "        model_ques = bot.ask(validator_guess_ans)\n",
    "        \n",
    "        if model_ques == \"I guessed it! Thanks for playing.\" or model_ques == \"I couldn't get it in 20 tries. You win!\":\n",
    "            if model_ques == \"I guessed it! Thanks for playing.\":\n",
    "                win += 1\n",
    "                True_guess.append(model_guess)\n",
    "            print(\"End of the game:\")\n",
    "            print(model_ques)\n",
    "            break\n",
    "    q_all.append(querries)\n",
    "\n",
    "# Statistic and results of games.\n",
    "\n",
    "print(f\"{win} games won out of {Number_of_games} games.\")\n",
    "print(\"True guesses: \",True_guess)\n",
    "\n",
    "# for qu in q_all:\n",
    "#     for q in qu:\n",
    "#         print(\"Model question: \", q['Model Question'])\n",
    "#         print(\"Validator answer: \", q['validator answer'])\n",
    "#         print(\"Model guess: \",q['Model guess'])\n",
    "#         print(\"Validator answer: \",q['Validator answer_g'])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7889443,
     "sourceId": 12500576,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
