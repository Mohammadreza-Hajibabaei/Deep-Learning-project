{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing modules","metadata":{}},{"cell_type":"code","source":"# ğŸ”§ 1) install (Ù‡Ù…ÙˆÙ† Ø®Ø· Ù‚Ø¨Ù„ÛŒ)\n!pip install --quiet transformers accelerate bitsandbytes sentence-transformers #faiss-cpu\n!pip install faiss-gpu-cu12","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:29:35.851918Z","iopub.execute_input":"2025-07-17T16:29:35.852568Z","iopub.status.idle":"2025-07-17T16:31:01.694263Z","shell.execute_reply.started":"2025-07-17T16:29:35.852548Z","shell.execute_reply":"2025-07-17T16:31:01.693366Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting faiss-gpu-cu12\n  Downloading faiss_gpu_cu12-1.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (25.0)\nRequirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (12.4.127)\nRequirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (12.4.5.8)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2->faiss-gpu-cu12) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2->faiss-gpu-cu12) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2->faiss-gpu-cu12) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2->faiss-gpu-cu12) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2->faiss-gpu-cu12) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2->faiss-gpu-cu12) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2->faiss-gpu-cu12) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2->faiss-gpu-cu12) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2->faiss-gpu-cu12) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2->faiss-gpu-cu12) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2->faiss-gpu-cu12) (2024.2.0)\nDownloading faiss_gpu_cu12-1.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (48.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.0/48.0 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-gpu-cu12\nSuccessfully installed faiss-gpu-cu12-1.11.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Section 1 : Chat bot core","metadata":{}},{"cell_type":"code","source":"# hugging fac token\n# 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","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T17:49:19.224906Z","iopub.execute_input":"2025-07-14T17:49:19.225203Z","iopub.status.idle":"2025-07-14T17:49:19.229069Z","shell.execute_reply.started":"2025-07-14T17:49:19.225181Z","shell.execute_reply":"2025-07-14T17:49:19.228406Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch, gc, os\nfrom huggingface_hub import login\n\nlogin(\"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\")\n\nmodel_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\ntokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n\nbnb_cfg = BitsAndBytesConfig(load_in_4bit=True,\n                             bnb_4bit_compute_dtype=torch.float16,\n                             bnb_4bit_use_double_quant=True,\n                             bnb_4bit_quant_type=\"nf4\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    quantization_config=bnb_cfg,\n    trust_remote_code=True,\n).eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:31:21.446370Z","iopub.execute_input":"2025-07-17T16:31:21.446680Z","iopub.status.idle":"2025-07-17T16:33:15.472492Z","shell.execute_reply.started":"2025-07-17T16:31:21.446649Z","shell.execute_reply":"2025-07-17T16:33:15.471891Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8100374276364ebfbaf1ef2548437adc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a79c4745399947cb8c23b45b170133d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a59d127c717b47ae8d1d8c6762243bd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07a7156a4e9a49c78f424b271c86da49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cffa2039e7b84ad6bbac9dc1f94c58b5"}},"metadata":{}},{"name":"stderr","text":"2025-07-17 16:31:41.040096: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752769901.384504      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752769901.482574      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61c440efa8f34f929e67d97d6b296496"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f930edcecbd6470997a34e16d677de36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98b467c24f034be381c2e2718677c891"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"331b743c51de4283b0126c7b40b9f900"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66f87b0f379b477ab3f1c5c4c7c4498e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b78caf31c894d8984dbf9a0903e5d40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2909efc8092040b7a3cb2e868f6320d0"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import json, gc, os, time\nfrom collections import deque\nfrom typing import List, Dict\n\n# ------------ tweakables -----------------------------------------------------\nSYSTEM_PROMPT = \"You are a helpful AI assistant.\"\nMAX_CTX_TOKENS   = 8000 - 512        # keep 512 tokens headroom\nSUMMARISE_AT_TOK = 6000              # start summarising above this\nCHUNK_SIZE       = 12                # summarise 12 oldest turns each time\nLOG_FILE         = \"chatlog.jsonl\"   # optional disk log\n# -----------------------------------------------------------------------------\n\ndef num_tokens(text: str) -> int:\n    # helper for quick token counting\n    return len(tokenizer.encode(text))\n\ndef chat_completion(messages: List[Dict],  # messages[-1] must be user\n                    max_new=256, temp=0.7, top_p=0.9):\n    prompt = tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    out = model.generate(\n        **inputs,\n        max_new_tokens=max_new,\n        temperature=temp,\n        top_p=top_p,\n        eos_token_id=tokenizer.eos_token_id\n    )\n    reply = tokenizer.decode(out[0][inputs.input_ids.shape[1]:],\n                             skip_special_tokens=True).strip()\n    return reply\n\n\nclass MemoryChatbot:\n    \"\"\"Keeps the last N turns verbatim and auto-summarises earlier ones.\"\"\"\n\n    def __init__(self,\n                 system_prompt: str = SYSTEM_PROMPT,\n                 max_ctx_tokens: int = MAX_CTX_TOKENS,\n                 summarise_at: int = SUMMARISE_AT_TOK,\n                 chunk_size: int = CHUNK_SIZE):\n        self.system_prompt = system_prompt\n        self.max_ctx_tokens = max_ctx_tokens\n        self.summarise_at   = summarise_at\n        self.chunk_size     = chunk_size\n\n        self.history = deque()    # list of {\"role\":..., \"content\":...}\n        self.memo    = \"\"         # running summary of trimmed turns\n\n    # ------------- public API -------------------------------------------------\n    def ask(self, user_msg: str) -> str:\n        \"\"\"Main entry: add user message â†’ maybe summarise â†’ get reply.\"\"\"\n        self._append(\"user\", user_msg)\n        self._maybe_summarise()\n        reply = self._generate_reply(user_msg)\n        self._append(\"assistant\", reply)\n        return reply\n    # -------------------------------------------------------------------------\n\n    # ------------- internal helpers ------------------------------------------\n    def _append(self, role, content):\n        self.history.append({\"role\": role, \"content\": content})\n        self._disk_log(role, content)\n\n    def _current_messages(self) -> List[Dict]:\n        msgs = [{\"role\": \"system\", \"content\": self.system_prompt}]\n        if self.memo:\n            msgs.append({\"role\": \"assistant\",\n                         \"content\": f\"[CONTEXT SUMMARY]\\n{self.memo}\"})\n        msgs.extend(self.history)\n        return msgs\n\n    def _prompt_tokens(self) -> int:\n        txt = tokenizer.apply_chat_template(self._current_messages(),\n                                            tokenize=False)\n        return num_tokens(txt)\n\n    def _maybe_summarise(self):\n        \"\"\"If conversation is getting heavy, summarise oldest chunk.\"\"\"\n        while self._prompt_tokens() > self.summarise_at and len(self.history) > self.chunk_size:\n            chunk = list(self.history)[:self.chunk_size]\n            chunk_txt = \"\\n\".join(f\"{m['role']}: {m['content']}\" for m in chunk)\n\n            summary_prompt = [\n                {\"role\": \"system\",\n                 \"content\": \"You are a summarisation assistant.\"},\n                {\"role\": \"user\",\n                 \"content\":\n                 (\"Summarise the following conversation in â‰¤8 bullet points, \"\n                  \"preserve all factual details:\\n\\n\" + chunk_txt)}\n            ]\n            summary = chat_completion(summary_prompt, max_new=160, temp=0.3)\n\n            # remove chunk & prepend summary\n            for _ in range(self.chunk_size):\n                self.history.popleft()\n            self.memo = (self.memo + \"\\n\" + summary).strip()\n\n            # free GPU RAM\n            gc.collect(); torch.cuda.empty_cache()\n\n            if self._prompt_tokens() < self.max_ctx_tokens:\n                break\n\n    def _generate_reply(self, user_msg):\n        msgs = self._current_messages()\n        reply = chat_completion(msgs)\n        gc.collect(); torch.cuda.empty_cache()\n        return reply\n\n    def _disk_log(self, role, content):\n        if LOG_FILE:\n            with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n                json.dump({\"ts\": time.time(), \"role\": role,\n                           \"content\": content}, f, ensure_ascii=False)\n                f.write(\"\\n\")\n    # -------------------------------------------------------------------------\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:33:33.484964Z","iopub.execute_input":"2025-07-17T16:33:33.485637Z","iopub.status.idle":"2025-07-17T16:33:33.499429Z","shell.execute_reply.started":"2025-07-17T16:33:33.485612Z","shell.execute_reply":"2025-07-17T16:33:33.498729Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ----------------------- quick demo ------------------------------------------\nbot = MemoryChatbot()\n\nqs = [\"Hey there! How are you?\",\n      \"Can you suggest two contemporary architecture books?\",\n      \"What chapters do those books include?\"]\n\nfor q in qs:\n    print(\"ğŸ‘¤\", q)\n    print(\"ğŸ¤–\", bot.ask(q), \"\\n\")\n\n# keep chatting â€¦ the bot will start summarising automatically","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Section 2: RAG","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport faiss\nfrom sentence_transformers import SentenceTransformer\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 1.  Install (once per session)\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n!pip install -q \"pymupdf>=1.22\" faiss-cpu sentence-transformers\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 2.  Build / load the FAISS index\n#     â€¢ Scans every *.pdf in /kaggle/input/pdf-folder\n#     â€¢ Extracts text with PyMuPDF\n#     â€¢ Splits it into â‰ˆ700â€‘character chunks\n#     â€¢ Embeds chunks with sentenceâ€‘transformers/allâ€‘MiniLMâ€‘L6â€‘v2\n#     â€¢ Saves index + metadata to /kaggle/working for reuse\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nimport os, glob, json, itertools, math, pathlib\nimport fitz                           # PyMuPDF\nimport faiss\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\n\nDATA_DIR   = \"/kaggle/input/pdf-folder\"\nINDEX_F    = \"/kaggle/working/rag.index\"\nMETA_F     = \"/kaggle/working/chunks.json\"\nCHUNK_SIZE = 700          # characters, â‰ˆ100â€¯words\n\ndef extract_text(pdf_path: str) -> str:\n    doc = fitz.open(pdf_path)\n    return \"\\n\".join(page.get_text() for page in doc)\n\ndef chunk_text(text: str, size: int = 700):\n    for start in range(0, len(text), size):\n        yield text[start : start + size]\n\ndef build_index():\n    pdf_files = sorted(glob.glob(os.path.join(DATA_DIR, \"*.pdf\")))\n    if not pdf_files:\n        raise FileNotFoundError(f\"No PDFs detected in {DATA_DIR}\")\n\n    chunks, meta = [], []\n    for path in pdf_files:\n        raw = extract_text(path)\n        for i, chunk in enumerate(chunk_text(raw, CHUNK_SIZE)):\n            meta.append({\"source\": os.path.basename(path), \"chunk_id\": i, \"text\": chunk})\n            chunks.append(chunk)\n\n    print(f\"âœ“ Extracted {len(chunks)} chunks from {len(pdf_files)} file(s).\")\n\n    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n    vecs  = model.encode(chunks, batch_size=32, show_progress_bar=True).astype(\"float32\")\n\n    index = faiss.IndexFlatL2(vecs.shape[1])\n    index.add(vecs)\n\n    faiss.write_index(index, INDEX_F)\n    with open(META_F, \"w\", encoding=\"utf-8\") as f:\n        json.dump(meta, f, ensure_ascii=False)\n\n    print(f\"âœ“ Index saved to {INDEX_F}; metadata to {META_F}\")\n\n# Build only if we have not done so already\nif not (pathlib.Path(INDEX_F).exists() and pathlib.Path(META_F).exists()):\n    build_index()\nelse:\n    print(\"Index already present â€“Â skipping rebuild.\")\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 3.  Lightweight retriever class (Sectionâ€¯2)\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nclass Retriever:\n    \"\"\"\n    â€¢ Filters out FAISS â€˜emptyâ€‘slotâ€™ returns (idÂ ==Â â€‘1, distanceÂ ==Â FLT_MAX)\n    â€¢ Converts L2 distance to cosineâ€‘like similarity in [0,â€¯1]\n    \"\"\"\n    def __init__(self, idx_path=\"/kaggle/working/rag.index\",\n                       meta_path=\"/kaggle/working/chunks.json\"):\n        self.model  = SentenceTransformer(\"all-MiniLM-L6-v2\")\n        self.index  = faiss.read_index(idx_path)\n        with open(meta_path, encoding=\"utf-8\") as f:\n            self.meta = json.load(f)\n        # preâ€‘compute norms once for the conversion formula\n        self._vec_norm = np.linalg.norm(\n            self.index.reconstruct(0) ).astype(\"float32\")  # all vectors same length\n\n    def _l2_to_similarity(self, l2: float) -> float:\n        # cosine_sim = 1 - (L2_distÂ²) / (2 * |a|Â²)   for unitâ€‘length queries â‰ˆ 1\n        return max(0.0, 1.0 - l2 / (2 * self._vec_norm**2))\n\n    def top_k(self, query: str, k: int = 3):\n        q_vec  = self.model.encode([query]).astype(\"float32\")\n        D, I   = self.index.search(q_vec, k)\n        hits   = []\n        for rank, (idx, dist) in enumerate(zip(I[0], D[0])):\n            if idx == -1 or np.isinf(dist) or dist > 1e8:\n                continue                          # FAISS padding â†’ skip\n            hits.append({\n                \"rank\"     : rank + 1,\n                \"source\"   : self.meta[idx][\"source\"],\n                \"chunk_id\" : self.meta[idx][\"chunk_id\"],\n                \"similarity\": round(self._l2_to_similarity(dist), 3),\n                \"text\"     : self.meta[idx][\"text\"].strip()\n            })\n        return hits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:56:50.959380Z","iopub.execute_input":"2025-07-17T16:56:50.959670Z","iopub.status.idle":"2025-07-17T16:56:54.258572Z","shell.execute_reply.started":"2025-07-17T16:56:50.959649Z","shell.execute_reply":"2025-07-17T16:56:54.257811Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Index already present â€“Â skipping rebuild.\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"retriever = Retriever()\nfor h in retriever.top_k(\"What nutrients do cats require?\", k=3):\n    print(f\"[{h['rank']}] sim={h['similarity']:.3f} â€¢ {h['source']} chunk {h['chunk_id']}\\n{h['text']}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:57:00.531893Z","iopub.execute_input":"2025-07-17T16:57:00.532188Z","iopub.status.idle":"2025-07-17T16:57:01.294353Z","shell.execute_reply.started":"2025-07-17T16:57:00.532163Z","shell.execute_reply":"2025-07-17T16:57:01.293732Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc513cb85b7e4ba8a5630f160fd9c218"}},"metadata":{}},{"name":"stdout","text":"[1] sim=0.566 â€¢ Cats.pdf chunk 0\nCats (Felis catus) \n   Basic Info: \nâ€¢ \nScientific Name: Felis catus \nâ€¢ \nLifespan: 12â€“18 years on average (can live over 20 years) \nâ€¢ \nWeight: Typically 3.5â€“5 kg (7â€“11 lbs), but varies by breed \nâ€¢ \nDomesticated: Around 9,000 years ago in the Near East \n   Behavior: \nâ€¢ \nNocturnal and crepuscular: Most active at dawn and dusk \nâ€¢ \nTerritorial: Mark with scent glands or scratching \nâ€¢ \nGrooming: Spend a lot of time cleaning their fur \nâ€¢ \nCommunication: Meowing (mostly for humans), purring, hissing, body language \n   Diet: \nâ€¢ \nObligate carnivores: Require a meat-based diet \nâ€¢ \nSensitive to diet imbalances: Need taurine, an amino acid found in animal tissue \n   Reproduction: \nâ€¢ \nGestation period:\n\n[2] sim=0.423 â€¢ Cats.pdf chunk 1\nAbout 63â€“65 days \nâ€¢ \nLitters: 2â€“5 kittens typically \n   Health: \nâ€¢ \nCommon diseases: Feline leukemia virus (FeLV), FIV, dental disease, kidney issues \nâ€¢ \nVaccines & vet care: Important for prevention \n   Breeds: \nâ€¢ \nHundreds of breeds; examples: \no Persian: Long-haired, calm \no Siamese: Sleek, vocal \no Maine Coon: Large, friendly \n\no Sphynx: Hairless \n   Interesting Facts: \nâ€¢ \nCats can rotate their ears 180 degrees. \nâ€¢ \nTheir whiskers help them detect nearby objects and measure gaps. \nâ€¢ \nA group of cats is called a clowder. \nâ€¢ \nCats can make over 100 different vocal sounds (dogs make about 10). \n \nWould you like information on a specific breed, health topic, or behavior?\n\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"from pathlib import Path\n\n# make sure the index was built\nassert Path(\"/kaggle/working/rag.index\").exists(), \"Run the buildâ€‘index cell first.\"\n\nretriever = Retriever(                       # â† class from Sectionâ€¯2\n    idx_path = \"/kaggle/working/rag.index\",\n    meta_path = \"/kaggle/working/chunks.json\"\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:55:49.373017Z","iopub.execute_input":"2025-07-17T16:55:49.373302Z","iopub.status.idle":"2025-07-17T16:55:50.173246Z","shell.execute_reply.started":"2025-07-17T16:55:49.373277Z","shell.execute_reply":"2025-07-17T16:55:50.172688Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"class RAGMemoryChatbot(MemoryChatbot):\n    \"\"\"\n    Adds Retrievalâ€‘Augmented Generation (RAG) on top of the Sectionâ€¯1 bot.\n    History handling, summarisation, and disk logging remain untouched.\n    \"\"\"\n    def __init__(self, retriever: Retriever, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.retriever = retriever\n\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def ask(self, user_msg: str, k: int = 3) -> str:\n        # 1) keep the normal bookkeeping\n        self._append(\"user\", user_msg)\n        self._maybe_summarise()\n\n        # 2) fetch topâ€‘k supporting passages (if any)\n        rag_block = \"\"\n        if self.retriever is not None:\n            hits = self.retriever.top_k(user_msg, k=k)\n            if hits:\n                rag_block = (\"\\n\\n---\\nRelevant background:\\n\" +\n                             \"\\n\\n\".join(f\"[Doc] {h['text']}\" for h in hits) +\n                             \"\\n---\")\n\n        # 3) build the prompt stack with the extra context\n        messages = self._current_messages()             # system + history\n        messages[0][\"content\"] += rag_block             # add to system prompt\n\n        # 4) call exactly the same LLM wrapper you already use\n        # reply = chat_completion(messages,\n        #                         model       = self.model_name,\n        #                         temperature = self.temperature)\n        print(messages)\n        reply = self._generate_reply(messages)\n        # 5) store assistant reply and continue as before\n        self._append(\"assistant\", reply)\n        return reply\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:55:58.802567Z","iopub.execute_input":"2025-07-17T16:55:58.803108Z","iopub.status.idle":"2025-07-17T16:55:58.809449Z","shell.execute_reply.started":"2025-07-17T16:55:58.803085Z","shell.execute_reply":"2025-07-17T16:55:58.808670Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"bot = RAGMemoryChatbot(retriever = retriever)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:56:01.609680Z","iopub.execute_input":"2025-07-17T16:56:01.609997Z","iopub.status.idle":"2025-07-17T16:56:01.613687Z","shell.execute_reply.started":"2025-07-17T16:56:01.609978Z","shell.execute_reply":"2025-07-17T16:56:01.612939Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"question = \"what is A + B?\"\nanswer   = bot.ask(question)            # same public method as before\nprint(answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:56:04.881636Z","iopub.execute_input":"2025-07-17T16:56:04.881936Z","iopub.status.idle":"2025-07-17T16:56:11.383568Z","shell.execute_reply.started":"2025-07-17T16:56:04.881915Z","shell.execute_reply":"2025-07-17T16:56:11.382574Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5b6d0ce32ab4ce9b27b6a558af8641c"}},"metadata":{}},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"[{'role': 'system', 'content': 'You are a helpful AI assistant.\\n\\n---\\nRelevant background:\\n[Doc] Cats (Felis catus) \\n   Basic Info: \\nâ€¢ \\nScientific Name: Felis catus \\nâ€¢ \\nLifespan: 12â€“18 years on average (can live over 20 years) \\nâ€¢ \\nWeight: Typically 3.5â€“5 kg (7â€“11 lbs), but varies by breed \\nâ€¢ \\nDomesticated: Around 9,000 years ago in the Near East \\n   Behavior: \\nâ€¢ \\nNocturnal and crepuscular: Most active at dawn and dusk \\nâ€¢ \\nTerritorial: Mark with scent glands or scratching \\nâ€¢ \\nGrooming: Spend a lot of time cleaning their fur \\nâ€¢ \\nCommunication: Meowing (mostly for humans), purring, hissing, body language \\n   Diet: \\nâ€¢ \\nObligate carnivores: Require a meat-based diet \\nâ€¢ \\nSensitive to diet imbalances: Need taurine, an amino acid found in animal tissue \\n   Reproduction: \\nâ€¢ \\nGestation period:\\n\\n[Doc] About 63â€“65 days \\nâ€¢ \\nLitters: 2â€“5 kittens typically \\n   Health: \\nâ€¢ \\nCommon diseases: Feline leukemia virus (FeLV), FIV, dental disease, kidney issues \\nâ€¢ \\nVaccines & vet care: Important for prevention \\n   Breeds: \\nâ€¢ \\nHundreds of breeds; examples: \\no Persian: Long-haired, calm \\no Siamese: Sleek, vocal \\no Maine Coon: Large, friendly \\n\\no Sphynx: Hairless \\n   Interesting Facts: \\nâ€¢ \\nCats can rotate their ears 180 degrees. \\nâ€¢ \\nTheir whiskers help them detect nearby objects and measure gaps. \\nâ€¢ \\nA group of cats is called a clowder. \\nâ€¢ \\nCats can make over 100 different vocal sounds (dogs make about 10). \\n \\nWould you like information on a specific breed, health topic, or behavior?\\n---'}, {'role': 'user', 'content': 'what is A + B?'}]\nTo answer your question, I would need to know what numbers A and B represent. In mathematics, the symbol \"+\" represents addition. So, A + B means the sum of the numbers A and B. For example, if A = 2 and B = 3, then A + B = 2 + 3 = 5.\n","output_type":"stream"}],"execution_count":37}]}