{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12500576,"sourceType":"datasetVersion","datasetId":7889443}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing modules","metadata":{}},{"cell_type":"code","source":"# ğŸ”§ 1) install (Ù‡Ù…ÙˆÙ† Ø®Ø· Ù‚Ø¨Ù„ÛŒ)\n!pip install --quiet transformers accelerate bitsandbytes sentence-transformers #faiss-cpu\n!pip install faiss-gpu-cu12","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T17:35:22.356496Z","iopub.execute_input":"2025-07-24T17:35:22.357145Z","iopub.status.idle":"2025-07-24T17:36:57.862216Z","shell.execute_reply.started":"2025-07-24T17:35:22.357118Z","shell.execute_reply":"2025-07-24T17:36:57.861549Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting faiss-gpu-cu12\n  Downloading faiss_gpu_cu12-1.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (25.0)\nRequirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (12.4.127)\nRequirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (12.4.5.8)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2->faiss-gpu-cu12) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2->faiss-gpu-cu12) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2->faiss-gpu-cu12) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2->faiss-gpu-cu12) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2->faiss-gpu-cu12) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2->faiss-gpu-cu12) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2->faiss-gpu-cu12) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2->faiss-gpu-cu12) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2->faiss-gpu-cu12) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2->faiss-gpu-cu12) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2->faiss-gpu-cu12) (2024.2.0)\nDownloading faiss_gpu_cu12-1.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (48.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.0/48.0 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-gpu-cu12\nSuccessfully installed faiss-gpu-cu12-1.11.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Section 1 : Chat bot core","metadata":{}},{"cell_type":"code","source":"# hugging fac token\n# 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","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T17:36:57.863732Z","iopub.execute_input":"2025-07-24T17:36:57.863983Z","iopub.status.idle":"2025-07-24T17:36:57.867635Z","shell.execute_reply.started":"2025-07-24T17:36:57.863959Z","shell.execute_reply":"2025-07-24T17:36:57.867089Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch, gc, os\nfrom huggingface_hub import login\n\nlogin(\"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\")\n\nmodel_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\ntokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n\nbnb_cfg = BitsAndBytesConfig(load_in_4bit=True,\n                             bnb_4bit_compute_dtype=torch.float16,\n                             bnb_4bit_use_double_quant=True,\n                             bnb_4bit_quant_type=\"nf4\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    quantization_config=bnb_cfg,\n    trust_remote_code=True,\n).eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T17:36:57.868419Z","iopub.execute_input":"2025-07-24T17:36:57.868669Z","iopub.status.idle":"2025-07-24T17:40:18.704869Z","shell.execute_reply.started":"2025-07-24T17:36:57.868647Z","shell.execute_reply":"2025-07-24T17:40:18.704125Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ace8ca0e48944b4becca9d1ab5c67cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67a20d890eb6452eb4f763372b218c47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"205ea1a1456b45dc8eb35b76eea655d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63cac774fffa462587106b7a66956c0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0aa9880b979b444b8a3e285989cfbfa7"}},"metadata":{}},{"name":"stderr","text":"2025-07-24 17:37:19.261593: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753378639.618576      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753378639.720945      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04617f10cff34cedb1521769d08070f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35a93d9d696b4c628d2330013b74fc57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a465445d960848f59954bc35f5b196f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd81a4388bb04270aede7d655e7ce9d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"341c7e7e4a57409881fbaf151f935763"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ca37e7b9f584c9197f84be9912ef197"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a7d2c93038e4c329d9fd72210347406"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import json, gc, os, time\nfrom collections import deque\nfrom typing import List, Dict\n\n# ------------ tweakables -----------------------------------------------------\nSYSTEM_PROMPT = \"You are a helpful AI assistant.\"\nMAX_CTX_TOKENS   = 8000 - 512        # keep 512 tokens headroom\nSUMMARISE_AT_TOK = 6000              # start summarising above this\nCHUNK_SIZE       = 12                # summarise 12 oldest turns each time\nLOG_FILE         = \"chatlog.jsonl\"   # optional disk log\n# -----------------------------------------------------------------------------\n\ndef num_tokens(text: str) -> int:\n    # helper for quick token counting\n    return len(tokenizer.encode(text))\n\ndef chat_completion(messages: List[Dict],  # messages[-1] must be user\n                    max_new=256, temp=0.7, top_p=0.9):\n    prompt = tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    out = model.generate(\n        **inputs,\n        max_new_tokens=max_new,\n        temperature=temp,\n        top_p=top_p,\n        eos_token_id=tokenizer.eos_token_id\n    )\n    reply = tokenizer.decode(out[0][inputs.input_ids.shape[1]:],\n                             skip_special_tokens=True).strip()\n    return reply\n\n\nclass MemoryChatbot:\n    \"\"\"Keeps the last N turns verbatim and auto-summarises earlier ones.\"\"\"\n\n    def __init__(self,\n                 system_prompt: str = SYSTEM_PROMPT,\n                 max_ctx_tokens: int = MAX_CTX_TOKENS,\n                 summarise_at: int = SUMMARISE_AT_TOK,\n                 chunk_size: int = CHUNK_SIZE):\n        self.system_prompt = system_prompt\n        self.max_ctx_tokens = max_ctx_tokens\n        self.summarise_at   = summarise_at\n        self.chunk_size     = chunk_size\n\n        self.history = deque()    # list of {\"role\":..., \"content\":...}\n        self.memo    = \"\"         # running summary of trimmed turns\n\n    # ------------- public API -------------------------------------------------\n    def ask(self, user_msg: str) -> str:\n        \"\"\"Main entry: add user message â†’ maybe summarise â†’ get reply.\"\"\"\n        self._append(\"user\", user_msg)\n        self._maybe_summarise()\n        reply = self._generate_reply(user_msg)\n        self._append(\"assistant\", reply)\n        return reply\n    # -------------------------------------------------------------------------\n\n    # ------------- internal helpers ------------------------------------------\n    def _append(self, role, content):\n        self.history.append({\"role\": role, \"content\": content})\n        self._disk_log(role, content)\n\n    def _current_messages(self) -> List[Dict]:\n        msgs = [{\"role\": \"system\", \"content\": self.system_prompt}]\n        if self.memo:\n            msgs.append({\"role\": \"assistant\",\n                         \"content\": f\"[CONTEXT SUMMARY]\\n{self.memo}\"})\n        msgs.extend(self.history)\n        return msgs\n\n    def _prompt_tokens(self) -> int:\n        txt = tokenizer.apply_chat_template(self._current_messages(),\n                                            tokenize=False)\n        return num_tokens(txt)\n\n    def _maybe_summarise(self):\n        \"\"\"If conversation is getting heavy, summarise oldest chunk.\"\"\"\n        while self._prompt_tokens() > self.summarise_at and len(self.history) > self.chunk_size:\n            chunk = list(self.history)[:self.chunk_size]\n            chunk_txt = \"\\n\".join(f\"{m['role']}: {m['content']}\" for m in chunk)\n\n            summary_prompt = [\n                {\"role\": \"system\",\n                 \"content\": \"You are a summarisation assistant.\"},\n                {\"role\": \"user\",\n                 \"content\":\n                 (\"Summarise the following conversation in â‰¤8 bullet points, \"\n                  \"preserve all factual details:\\n\\n\" + chunk_txt)}\n            ]\n            summary = chat_completion(summary_prompt, max_new=160, temp=0.3)\n\n            # remove chunk & prepend summary\n            for _ in range(self.chunk_size):\n                self.history.popleft()\n            self.memo = (self.memo + \"\\n\" + summary).strip()\n\n            # free GPU RAM\n            gc.collect(); torch.cuda.empty_cache()\n\n            if self._prompt_tokens() < self.max_ctx_tokens:\n                break\n\n    def _generate_reply(self, user_msg):\n        msgs = self._current_messages()\n        reply = chat_completion(msgs)\n        gc.collect(); torch.cuda.empty_cache()\n        return reply\n\n    def _disk_log(self, role, content):\n        if LOG_FILE:\n            with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n                json.dump({\"ts\": time.time(), \"role\": role,\n                           \"content\": content}, f, ensure_ascii=False)\n                f.write(\"\\n\")\n    # -------------------------------------------------------------------------\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T17:40:18.706768Z","iopub.execute_input":"2025-07-24T17:40:18.707686Z","iopub.status.idle":"2025-07-24T17:40:18.721012Z","shell.execute_reply.started":"2025-07-24T17:40:18.707661Z","shell.execute_reply":"2025-07-24T17:40:18.720495Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# # ----------------------- quick demo ------------------------------------------\n# bot = MemoryChatbot()\n\n# qs = [\"Hey there! How are you?\",\n#       \"Can you suggest two contemporary architecture books?\",\n#       \"What chapters do those books include?\"]\n\n# for q in qs:\n#     print(\"ğŸ‘¤\", q)\n#     print(\"ğŸ¤–\", bot.ask(q), \"\\n\")\n\n# # keep chatting â€¦ the bot will start summarising automatically","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T17:40:18.721639Z","iopub.execute_input":"2025-07-24T17:40:18.721878Z","iopub.status.idle":"2025-07-24T17:40:18.743133Z","shell.execute_reply.started":"2025-07-24T17:40:18.721861Z","shell.execute_reply":"2025-07-24T17:40:18.742595Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Section 2: RAG","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport faiss\nfrom sentence_transformers import SentenceTransformer\nimport re   # â† add near the other imports\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 1.  Install (once per session)\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n!pip install -q \"pymupdf>=1.22\" faiss-cpu sentence-transformers\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 2.  Build / load the FAISS index\n#     â€¢ Scans every *.pdf in /kaggle/input/pdf-folder\n#     â€¢ Extracts text with PyMuPDF\n#     â€¢ Splits it into â‰ˆ700â€‘character chunks\n#     â€¢ Embeds chunks with sentenceâ€‘transformers/allâ€‘MiniLMâ€‘L6â€‘v2\n#     â€¢ Saves index + metadata to /kaggle/working for reuse\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nimport os, glob, json, itertools, math, pathlib\nimport fitz                           # PyMuPDF\nimport faiss\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\n\nDATA_DIR   = \"/kaggle/input/pdf-folder\"\nINDEX_F    = \"/kaggle/working/rag.index\"\nMETA_F     = \"/kaggle/working/chunks.json\"\nCHUNK_SIZE = 700          # characters, â‰ˆ100â€¯words\n\ndef extract_text(pdf_path: str) -> str:\n    doc = fitz.open(pdf_path)\n    return \"\\n\".join(page.get_text() for page in doc)\n\ndef chunk_text(text: str, size: int = 700):\n    for start in range(0, len(text), size):\n        yield text[start : start + size]\n\ndef clean(text: str) -> str:\n    \"\"\"\n    Normalises the text so embeddings are not polluted by layout artefacts.\n\n    â€¢ replaces hard lineâ€‘breaks (\\n, \\r) with a single space\n    â€¢ keeps only ASCII letters, digits, and whitespace\n    â€¢ collapses 2+ whitespace chars into one space\n    \"\"\"\n    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n    return text    \n    # text = re.sub(r\"[^A-Za-z0-9\\s]+\", \" \", text)   # strip punctuation + accents\n    # return re.sub(r\"\\s{2,}\", \" \", text).strip()\n\n\ndef build_index():\n    pdf_files = sorted(glob.glob(os.path.join(DATA_DIR, \"*.pdf\")))\n    if not pdf_files:\n        raise FileNotFoundError(f\"No PDFs detected in {DATA_DIR}\")\n\n    chunks, meta = [], []\n    for path in pdf_files:\n        raw_text = extract_text(path)               # <-- original extractor\n        raw_text = clean(raw_text)                  # <-- NEW: sanitise once\n        for i, chunk in enumerate(chunk_text(raw_text, CHUNK_SIZE)):\n            meta.append({\"source\": os.path.basename(path), \"chunk_id\": i, \"text\": chunk})\n            chunks.append(chunk)\n\n    print(f\"âœ“ Extracted {len(chunks)} chunks from {len(pdf_files)} file(s).\")\n\n    embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n    vecs  = embed_model.encode(chunks, batch_size=32, show_progress_bar=True).astype(\"float32\")\n\n    index = faiss.IndexFlatL2(vecs.shape[1])\n    index.add(vecs)\n\n    faiss.write_index(index, INDEX_F)\n    with open(META_F, \"w\", encoding=\"utf-8\") as f:\n        json.dump(meta, f, ensure_ascii=False)\n\n    print(f\"âœ“ Index saved to {INDEX_F}; metadata to {META_F}\")\n\n# Build only if we have not done so already\n# if not (pathlib.Path(INDEX_F).exists() and pathlib.Path(META_F).exists()):\n#     build_index()\n# else:\n#     print(\"Index already present â€“Â skipping rebuild.\")\nbuild_index()\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 3.  Lightweight retriever class (Sectionâ€¯2)\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nclass Retriever:\n    \"\"\"\n    â€¢ Filters out FAISS â€˜emptyâ€‘slotâ€™ returns (idÂ ==Â â€‘1, distanceÂ ==Â FLT_MAX)\n    â€¢ Converts L2 distance to cosineâ€‘like similarity in [0,â€¯1]\n    \"\"\"\n    def __init__(self, idx_path=\"/kaggle/working/rag.index\",\n                       meta_path=\"/kaggle/working/chunks.json\"):\n        self.model  = SentenceTransformer(\"all-MiniLM-L6-v2\")\n        self.index  = faiss.read_index(idx_path)\n        with open(meta_path, encoding=\"utf-8\") as f:\n            self.meta = json.load(f)\n        # preâ€‘compute norms once for the conversion formula\n        self._vec_norm = np.linalg.norm(\n            self.index.reconstruct(0) ).astype(\"float32\")  # all vectors same length\n\n    def _l2_to_similarity(self, l2: float) -> float:\n        # cosine_sim = 1 - (L2_distÂ²) / (2 * |a|Â²)   for unitâ€‘length queries â‰ˆ 1\n        return max(0.0, 1.0 - l2 / (2 * self._vec_norm**2))\n\n    def top_k(self, query: str, k: int = 3):\n        q_vec  = self.model.encode([query]).astype(\"float32\")\n        D, I   = self.index.search(q_vec, k)\n        hits   = []\n        for rank, (idx, dist) in enumerate(zip(I[0], D[0])):\n            if idx == -1 or np.isinf(dist) or dist > 1e8:\n                continue                          # FAISS padding â†’ skip\n            hits.append({\n                \"rank\"     : rank + 1,\n                \"source\"   : self.meta[idx][\"source\"],\n                \"chunk_id\" : self.meta[idx][\"chunk_id\"],\n                \"similarity\": round(self._l2_to_similarity(dist), 3),\n                \"text\"     : self.meta[idx][\"text\"].strip()\n            })\n        return hits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T17:40:18.743907Z","iopub.execute_input":"2025-07-24T17:40:18.744153Z","iopub.status.idle":"2025-07-24T17:40:39.581157Z","shell.execute_reply.started":"2025-07-24T17:40:18.744127Z","shell.execute_reply":"2025-07-24T17:40:39.580504Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hâœ“ Extracted 2 chunks from 1 file(s).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c99668deece346d9ab669523e5a2aa29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9644e10cbda4beaba333340beeb7b9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81991af868774ed3912d3b69768d70fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34673171b0bf4618be392087a60ff140"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccff060a26fe45d68fce281aed58449d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac26f04580f44d47ba78107dbd974816"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57514eb2c11f40c68a34d3cf142a2e1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eecb1769d4c74f76849d4c67104b3f96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af163029336f47e880247c776ecf1021"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23a4795b17864ea685fb53ca26f02bae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2600bb30409d421590ac1222a4afed51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04640444db3f48b7bad70e854f97e63e"}},"metadata":{}},{"name":"stdout","text":"âœ“ Index saved to /kaggle/working/rag.index; metadata to /kaggle/working/chunks.json\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# retriever = Retriever()\n# for h in retriever.top_k(\"What is A + B + C?\", k=3):\n#     print(f\"[{h['rank']}] sim={h['similarity']:.3f} â€¢ {h['source']} chunk {h['chunk_id']}\\n{h['text']}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T17:40:39.581928Z","iopub.execute_input":"2025-07-24T17:40:39.582207Z","iopub.status.idle":"2025-07-24T17:40:39.585388Z","shell.execute_reply.started":"2025-07-24T17:40:39.582173Z","shell.execute_reply":"2025-07-24T17:40:39.584803Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from pathlib import Path\n\n# make sure the index was built\nassert Path(\"/kaggle/working/rag.index\").exists(), \"Run the buildâ€‘index cell first.\"\n\nretriever = Retriever(                       # â† class from Sectionâ€¯2\n    idx_path = \"/kaggle/working/rag.index\",\n    meta_path = \"/kaggle/working/chunks.json\"\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T17:40:39.586067Z","iopub.execute_input":"2025-07-24T17:40:39.586382Z","iopub.status.idle":"2025-07-24T17:40:40.810066Z","shell.execute_reply.started":"2025-07-24T17:40:39.586354Z","shell.execute_reply":"2025-07-24T17:40:40.809510Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class RAGMemoryChatbot(MemoryChatbot):\n    \"\"\"\n    Adds Retrievalâ€‘Augmented Generation (RAG) on top of the Sectionâ€¯1 bot.\n    History handling, summarisation, and disk logging remain untouched.\n    \"\"\"\n    def __init__(self, retriever: Retriever, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.retriever = retriever\n        \n    def ask(self, user_msg: str, k: int = 3) -> str:\n        # 1) keep the normal bookkeeping\n        self._append(\"user\", user_msg)\n        self._maybe_summarise()\n\n        # 2) fetch topâ€‘k supporting passages (if any)\n        rag_block = \"\"\n        if self.retriever is not None:\n            hits = self.retriever.top_k(user_msg, k=k)\n            if hits:\n                rag_block = (\"Relevant background:\" .join(f\"[doc] {h['text']}\" for h in hits))\n                # rag_block = (\"\\n\\n---\\nRelevant background:\\n\" +\n                #              \"\\n\\n\".join(f\"[Doc] {h['text']}\" for h in hits) +\n                #              \"\\n---\")\n\n        # 3) build the prompt stack with the extra context\n        messages = self._current_messages()             # system + history\n        messages[0][\"content\"] += rag_block             # add to system prompt\n        \n        # 4) call exactly the same LLM wrapper you already use\n        reply = chat_completion(messages,temp = 0.2)\n        \n        # 5) store assistant reply and continue as before\n        self._append(\"assistant\", reply)\n        return reply\n        \n    # def ask(self, user_msg: str, k: int = 3) -> str:\n    #     \"\"\"Main entry: add user message â†’ maybe summarise â†’ get reply.\"\"\"\n    #     rag_block = \"\"\n    #     if self.retriever is not None:\n    #         hits = self.retriever.top_k(user_msg, k=k)\n    #         if hits:\n    #             rag_block = (\"Relevant background:\" .join(f\"{h['text']}\" for h in hits))\n    #             # rag_block = (\"\\n\\n---\\nRelevant background:\\n\" +\n    #             #              \"\\n\\n\".join(f\"[Doc] {h['text']}\" for h in hits) +\n    #             #              \"\\n---\")\n    #     # print(rag_block)\n    #     # print(self._current_messages())\n        \n    #     # messages = self._current_messages()             # system + history\n    #     # messages[0][\"content\"] += rag_block             # add to system prompt\n        \n    #     self._append(\"user\", rag_block)\n    #     self._append(\"user\", user_msg)\n    #     # print(self._current_messages())\n    #     self._maybe_summarise()\n        \n    #     reply = self._generate_reply(user_msg)\n    #     self._append(\"assistant\", reply)\n    #     return reply\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nbot = RAGMemoryChatbot(retriever = retriever)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T17:40:40.810899Z","iopub.execute_input":"2025-07-24T17:40:40.811173Z","iopub.status.idle":"2025-07-24T17:40:40.817868Z","shell.execute_reply.started":"2025-07-24T17:40:40.811155Z","shell.execute_reply":"2025-07-24T17:40:40.817341Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# question = \"There are some information in doc about amount of A,B and C and it is not related to cat. According to this can you give me the result of (A+B)\"\nquestion = \"what is information about persian cat in doc?\"\nanswer   = bot.ask(question)            # same public method as before\nprint(answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T17:40:40.819959Z","iopub.execute_input":"2025-07-24T17:40:40.820187Z","iopub.status.idle":"2025-07-24T17:40:51.807894Z","shell.execute_reply.started":"2025-07-24T17:40:40.820164Z","shell.execute_reply":"2025-07-24T17:40:51.807091Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd52527cf2334e4283f1fce359a0e56c"}},"metadata":{}},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"The document provides some basic information about cats in general, and then specifically mentions the Persian cat breed. Here's the information related to Persian cats from the document:\n\n* Persian: Long-haired, calm\n\nThis suggests that Persian cats are a long-haired breed of cat known for their calm demeanor. No other specific details about the Persian breed are mentioned in the provided document. However, it does mention that there are hundreds of cat breeds, and Persian is just one example.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Section 3 : Function call","metadata":{}},{"cell_type":"markdown","source":"Part 3.1","metadata":{}},{"cell_type":"code","source":"!pip install exa-py\n!pip install openai\n!pip install python-dotenv\n%env EXA_API_KEY=af19a97b-45de-4ab4-8344-7029c5b7e7d6","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T17:40:51.808767Z","iopub.execute_input":"2025-07-24T17:40:51.809002Z","iopub.status.idle":"2025-07-24T17:41:01.769774Z","shell.execute_reply.started":"2025-07-24T17:40:51.808987Z","shell.execute_reply":"2025-07-24T17:41:01.769045Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting exa-py\n  Downloading exa_py-1.14.18-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from exa-py) (0.28.1)\nRequirement already satisfied: openai>=1.48 in /usr/local/lib/python3.11/dist-packages (from exa-py) (1.91.0)\nRequirement already satisfied: pydantic>=2.10.6 in /usr/local/lib/python3.11/dist-packages (from exa-py) (2.11.7)\nRequirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from exa-py) (2.32.4)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from exa-py) (4.14.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->exa-py) (4.9.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->exa-py) (2025.6.15)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->exa-py) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->exa-py) (3.10)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.28.1->exa-py) (0.16.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.48->exa-py) (1.9.0)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.48->exa-py) (0.10.0)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.48->exa-py) (1.3.1)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>=1.48->exa-py) (4.67.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.6->exa-py) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.6->exa-py) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.6->exa-py) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.3->exa-py) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.3->exa-py) (2.5.0)\nDownloading exa_py-1.14.18-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: exa-py\nSuccessfully installed exa-py-1.14.18\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.91.0)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\nRequirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting python-dotenv\n  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\nDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\nInstalling collected packages: python-dotenv\nSuccessfully installed python-dotenv-1.1.1\nenv: EXA_API_KEY=af19a97b-45de-4ab4-8344-7029c5b7e7d6\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Section 3 â€“ Web-augmented RAG + Memory\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nimport requests, os, textwrap, gc, torch\nfrom exa_py import Exa\n\nEXA_ENDPOINT = \"https://api.exa.ai/search\"       # Ø«Ø§Ø¨ØªØ› ØªØºÛŒÛŒØ± Ù†Ø¯Ù‡ÛŒØ¯\n\nclass WebRAGMemoryChatbot(RAGMemoryChatbot):\n    \"\"\"\n    RAG + Memory + ØªØ´Ø®ÛŒØµ Ø®ÙˆØ¯Ú©Ø§Ø± Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¬Ø³Øªâ€ŒÙˆØ¬ÙˆÛŒ ÙˆØ¨ (Part 3.1)\n    --------------------------------------------------------------------------\n    â€¢ inherits history, summarisation, and local-PDF RAG from Section 2\n    â€¢ adds:\n        â€“ _needs_web_search   â†’ yes/no (Ø¨Ø§ Ù‡Ù…Ø§Ù† LLM)\n        â€“ _exa_search         â†’ 3 hits  (Exa API)\n        â€“ _summarise_hits     â†’ â‰¤120-word digest\n    \"\"\"\n    # ---------- init ---------------------------------------------------------\n    def __init__(self, exa_api_key: str, *args, **kwargs):\n        super().__init__(*args, **kwargs)         # Ø§Ø² RAGMemoryChatbot\n        self.exa_api_key = exa_api_key\n        self.exa = Exa(exa_api_key)   # â† ÛŒÚ©â€ŒØ¨Ø§Ø± Ø³Ø§Ø®ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯\n    # ---------- helpers ------------------------------------------------------\n    def _needs_web_search(self, user_msg: str) -> bool:\n        prompt = [\n            {\"role\": \"system\",\n             \"content\": (\"You are a classifier that decides whether answering \"\n                         \"the user requires fresh external information. \"\n                         \"Reply with exactly 'yes' or 'no'.\")},\n            {\"role\": \"user\", \"content\": user_msg}\n        ]\n        out = chat_completion(prompt, max_new=1, temp=0).strip().lower()\n        return out.startswith(\"y\")      # â€œyesâ€ â†’ True\n    \n    def _exa_search(self, query: str, k: int = 3):\n        \"\"\"\n        Uses exa_py.search_and_contents â†’ one call does both search + content\n        Returns a list[{title,url,summary}] for downstream summarisation.\n        \"\"\"\n        if not self.exa_api_key:\n            raise ValueError(\"âŒ EXA_API_KEY is empty or not set.\")\n    \n        # call the SDK â€“ we want full page text, not just metadata\n        response = self.exa.search_and_contents(\n            query,\n            text=True,                # full text of each result\n            num_results=k,             # â† snake_case in SDK\n            # contex = True,\n            summary = True\n        )                              # :contentReference[oaicite:0]{index=0}\n        # print(response)\n        hits = []\n        for r in response.results:     # ResultWithText objects\n            hits.append({\n                \"title\":   r.title,\n                \"url\":     r.url,\n                \"text\": (r.text or \"\")#[:600]      # first 400 chars\n                ,\"summary\": r.summary\n            })\n        return hits\n\n\n    def _summarise_hits(self, hits) -> str:\n        joined = \"\\n\".join(f\"â€¢ {h['title']}: {h['summary']}\" for h in hits)\n        prompt = [\n            {\"role\": \"system\",\n             \"content\": \"Summarise the following web snippets in â‰¤120 words.\"},\n            {\"role\": \"user\", \"content\": joined}\n        ]\n        return chat_completion(prompt, max_new=160, temp=0.3).strip()\n\n\n    # ---------- public API ---------------------------------------------------\n    def ask(self, user_msg: str, k: int = 3) -> str:\n        # 1) bookkeeping + Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø®Ù„Ø§ØµÙ‡Ù” Ø­Ø§ÙØ¸Ù‡ ÙØ¹Ø§Ù„ Ø´ÙˆØ¯\n        self._append(\"user\", user_msg)\n        self._maybe_summarise()\n\n        # 2) RAG Ø¢ÙÙ„Ø§ÛŒÙ† (PDF)\n        rag_block = \"\"\n        if self.retriever is not None:\n            hits = self.retriever.top_k(user_msg, k=k)\n            if hits:\n                rag_block = (\"\\n\\n---\\n[Local Docs]\\n\" +\n                             \"\\n\\n\".join(f\"{h['text']}\" for h in hits) +\n                             \"\\n---\")\n\n        # 3) Web-Trigger\n        web_block = \"\"\n        if self._needs_web_search(user_msg):\n            try:\n                web_hits = self._exa_search(user_msg, k=3)\n                summary  = self._summarise_hits(web_hits)\n                web_block = f\"\\n\\n---\\n[Web Info]\\n{summary}\\n---\"\n                # print(\"earch is done\")\n            except Exception as e:\n                print(\"âš ï¸  Web search failed:\", e)\n        \n        # summaries = \"\"\n        # for h in web_hits:\n        #     summaries+= h[\"summary\"] \n        # web_block += summaries\n        # web_block += \"---\"\n        # print(web_block)\n        # 4) build full prompt & get answer\n        messages = self._current_messages()        # system + history (+memo)\n        messages[0][\"content\"] += rag_block + web_block #+ \"\\nGive user the latest and newst information.\"\n        reply = chat_completion(messages)\n\n        # 5) save reply\n        self._append(\"assistant\", reply)\n        return reply\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T17:41:01.771054Z","iopub.execute_input":"2025-07-24T17:41:01.771354Z","iopub.status.idle":"2025-07-24T17:41:03.618896Z","shell.execute_reply.started":"2025-07-24T17:41:01.771329Z","shell.execute_reply":"2025-07-24T17:41:03.618336Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"bot = WebRAGMemoryChatbot(\n        exa_api_key=os.getenv(\"EXA_API_KEY\"),\n        retriever=retriever,\n        system_prompt=SYSTEM_PROMPT\n)\n# q = \"What is the weather like in Tehran today?\"\n# q = \"What date is it today in tehran?\"\n# q = \"what news are about Syria today?\"\n# q = \"What is the price of gold in dollars today?\"\n# q = \"Who is the president of Iran right now?\"\n# q = \"What new news is about Thailand?\"\nq = \"What is (a*b)+d - s if a = 1,b = 2,d = 4, s = 10\"\nanswer = bot.ask(q)\nprint(answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T17:41:03.619550Z","iopub.execute_input":"2025-07-24T17:41:03.619727Z","iopub.status.idle":"2025-07-24T17:41:40.511396Z","shell.execute_reply.started":"2025-07-24T17:41:03.619713Z","shell.execute_reply":"2025-07-24T17:41:40.510589Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71dd0b0dc962457d8f999471fc0eb0bc"}},"metadata":{}},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"The expression (a*b) + d - s with given values a = 1, b = 2, d = 4, and s = 10 can be calculated as follows:\n\n1. Multiply a and b: 1 * 2 = 2\n2. Multiply the result with d: 2 * 4 = 8\n3. Subtract s from the result: 8 - 10 = -2\n\nSo, the answer is -2.\n\nHowever, the expression given in the \"Solved\" snippet is different: 5c - 3d + 11. The solution for that expression with given values c = 7 and d = 8 is:\n\n1. Multiply d with -3: -3 * 8 = -24\n2. Multiply 5 with c: 5 * 7 = 35\n3. Subtract the result from -24: 35 - (-24) = 61\n4. Add 11 to the result: 61 + 11 = 72\n\nSo, the solution for the expression 5c\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"Part 3.2","metadata":{}},{"cell_type":"code","source":"# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Game-aware Web-RAG + Memory Chatbot  (20-Questions, EN-only)\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nimport re, json, textwrap, random\nfrom exa_py import Exa        # already used in WebRAGMemoryChatbot\n\nQUESTION_FALLBACKS = [\n    \"Is it man-made?\", \"Is it bigger than a loaf of bread?\",\n    \"Is it commonly found indoors?\", \"Is it electronic?\"\n]\n\nGUESS_FALLBACKS = [\"cat\", \"dog\", \"tree\", \"car\", \"phone\", \"apple\", \"lion\"]\n\nBAD_GUESSES = {\"\", \"thing\", \"object\", \"based\"}\n\nGENERAL_QUESTIONS = [\n    \"Is it alive?\",\n    \"Is it man-made?\",\n    \"Is it bigger than a loaf of bread?\",\n    \"Is it something you can hold in one hand?\",\n    \"Is it commonly found indoors?\",\n    \"Is it electronic?\",\n    \"Is it primarily used for entertainment?\",\n    \"Is it used for communication?\",\n    \"Is it edible?\",\n    \"Was it invented after 1900?\",\n    \"Is it a proper noun?\",\n    \"Is it a place?\",\n    \"Is it a brand?\",\n    \"Is it a living animal?\",\n    \"Is it a living plant?\",\n    \"Is it a mineral or gemstone?\",\n    \"Does it require electricity to function?\",\n    \"Is it typically found outdoors?\",\n    \"Is it associated with technology?\",\n    \"Is it larger than a car?\"\n]\nclass GameWebRAGMemoryChatbot(WebRAGMemoryChatbot):\n    # --------------------------------------------------------------------- #\n    # 0. constructor & single-point game-state init\n    # --------------------------------------------------------------------- #\n    def __init__(self, exa_api_key: str, *args, **kwargs):\n        super().__init__(exa_api_key, *args, **kwargs)\n        self._init_game_state()\n\n    def _init_game_state(self):\n        self.game_active     = False\n        self.phase           = \"ask\"     # \"ask\" | \"guess\"\n        self.questions_asked = 0\n        self.max_questions   = 20\n        self.history_qna     = []        # [(question, answer), â€¦]\n        self._asked_set      = set()     # lower-cased questions\n        self._guess_set      = set()     # lower-cased guesses\n    # --------------------------------------------------------------------- #\n    # 1. intent detection helpers\n    # --------------------------------------------------------------------- #\n    def _detect_game_start(self, user_msg: str) -> bool:\n        prompt = [\n            {\"role\": \"system\",\n             \"content\": (\"You are a classifier. Reply with 'yes' if the \"\n                         \"user wants to play the 20-Questions game; \"\n                         \"otherwise 'no'.\")},\n            {\"role\": \"user\", \"content\": user_msg}\n        ]\n        return chat_completion(prompt, max_new=1, temp=0).strip().lower().startswith(\"y\")\n\n    def _detect_quit(self, user_msg: str) -> bool:\n        return bool(re.search(r\"\\b(quit|exit|cancel|stop|end game)\\b\",\n                              user_msg, flags=re.I))\n    # --------------------------------------------------------------------- #\n    # 2. question / guess generators  (duplicate-safe)\n    # --------------------------------------------------------------------- #\n    def _generate_question(self) -> str:\n        # ----------------------------------------- #\n        # 0) deterministic broad questions first\n        # ----------------------------------------- #\n        # for q in GENERAL_QUESTIONS:\n        #     if q.lower() not in self._asked_set:\n        #         return q\n    \n        # ----------------------------------------- #\n        # 1) ask LLM for a NEW yes/no question\n        #    (after weâ€™ve exhausted the broad list)\n        # ----------------------------------------- #\n        context = \"\\n\".join(f\"Q: {q} | A: {a}\"\n                            for q, a in self.history_qna[-8:]\n                            if a is not None) or \"[start]\"\n        forbid  = \"; \".join(sorted(self._asked_set))\n        sys = (\n            \"You are the Question-Asker in 20-Questions.\\n\"\n            \"Return ONE yes/no question, â‰¤12 words, no commas, no parentheses.\\n\"\n            \"Ask general yes/no question if list of yes/no questions is little.\\n\"\n            f\"Never repeat any of these yes/no questions : {forbid or '[none]'}\\n\"\n            f\"Consider these yes/no questions According to answers for making new relatable question but don't use these question again or part of them:{context}\"\n        )\n\n        prompt = [{\"role\": \"system\", \"content\": sys},\n                  {\"role\": \"user\",   \"content\": context}]\n        for _ in range(5):\n            q = chat_completion(prompt).strip()\n            q = q.split(\"?\")[0].strip().capitalize() + \"?\"\n            if q.lower() not in self._asked_set:# and len(q.split()) <= 12:\n                return q\n    \n        # ----------------------------------------- #\n        # 2) final static fallback (wonâ€™t duplicate)\n        # ----------------------------------------- #\n        for fb in QUESTION_FALLBACKS:\n            if fb.lower() not in self._asked_set:\n                return fb\n        return \"Is it tangible?\"\n\n\n    def _generate_guess(self) -> str:\n        context = \"     \".join(f\"Q: {q} | Answer: {a}\" for q, a in self.history_qna)\n        forbid  = \", \".join(sorted(self._guess_set))\n        sys = (\n            \"You are the ONE-WORD Guesser in 20-Questions game.          \"\n            # \"as your best current guess, never a word like 'thing' or 'based'.\\n\"\n            #You Already guessed these ONE-WORD and they are NOT the ANSWER and don't use them again but you can use related ONE-WORD if it is needed\n            f\"Make your ONE-WORD guess considering these questions and their answers from user: {context or '-'}.         \"\n            f\"These words ->> {forbid or '-'} <<- are NOT answer, Guess something else.         \"\n            # f\"Your ONE-WORD guess must not be one of these words: {BAD_GUESSES}.              \"\n            # \"Answer with ONE WORD and NEVER explain anything more.\"\n            \">>>>>>>Give your final answer in one word at first with NO EXPLANATION.\"\n            # \"answer with ONE WORD and Never explain anything more.\"\n            \"Reply with ONE lowercase English noun (no spaces, no punctuation).<<<<<<< \"\n        )\n        prompt = [{\"role\": \"system\", \"content\": sys},\n                  {\"role\": \"user\",   \"content\": context or \"[start]\"}]\n        # print(50*\"-\")\n        # print(\"Prompt: \", prompt)\n        # print(\"history: \",self.history_qna)\n        # print(\"asked set: \",self._asked_set)\n        # print(\"Guess set: \",self._guess_set)\n        # print(\"dorbid: \",forbid)\n        # print(\"sys: \",sys)\n        # print(\"context: \",context)\n        # print(50*\"-\")\n        for _ in range(5):\n            g = chat_completion(prompt)\n            print(g)\n            g = g.strip().split()[0]\n            # g = chat_completion(prompt, max_new=4, temp=0.7).strip().split()[0]\n            g = re.sub(r\"[^a-zA-Z]\", \"\", g).lower()\n            # if g and g not in self._guess_set and g not in BAD_GUESSES:\n            # print(g)\n            # if (g not in self._guess_set) and (g not in BAD_GUESSES):\n            if (g not in self._guess_set):\n                return g\n        for fb in GUESS_FALLBACKS:\n            if fb not in self._guess_set:\n                return fb\n        return \"idea\"\n    # --------------------------------------------------------------------- #\n    # 3. public ask()  (state machine)\n    # --------------------------------------------------------------------- #\n    def ask(self, user_msg: str, k: int = 3) -> str:\n        # -------- escape hatch --------\n        if self.game_active and self._detect_quit(user_msg):\n            self._init_game_state()\n            return \"Game stopped. Say 'play 20 questions' to start again.\"\n        # -------- active game ---------\n        if self.game_active:\n            # user answered a QUESTION  â†’ now make a GUESS\n            if self.phase == \"ask\":\n                self.history_qna[-1] = (self.history_qna[-1][0], user_msg.strip())\n                guess = self._generate_guess()\n                self._guess_set.add(guess.lower())\n                self.phase = \"guess\"\n                return f\"Is it **{guess}**? (Yes/No)\"\n            # user judged our GUESS\n            if self.phase == \"guess\":\n                if user_msg.lower().startswith(\"yes\"):\n                    self._init_game_state()\n                    return \"ğŸ‰ I guessed it! Thanks for playing.\"\n                # wrong guess\n                self.questions_asked += 1\n                if self.questions_asked >= self.max_questions:\n                    self._init_game_state()\n                    return \"ğŸ˜” I couldn't get it in 20 tries. You win!\"\n                # next question\n                q = self._generate_question()\n                self.history_qna.append((q, None))\n                self._asked_set.add(q.lower())\n                self.phase = \"ask\"\n                return q + \" (Yes/No)\"\n        # -------- start trigger -------\n        if self._detect_game_start(user_msg):\n            self._init_game_state()\n            self.game_active = True\n            first_q = self._generate_question()\n            self.history_qna.append((first_q, None))\n            self._asked_set.add(first_q.lower())\n            intro = (\"Let's play 20 Questions! Think of a word; \"\n                     \"I'll guess in â‰¤20 yes/no questions.\")\n            return intro + \"\\n\\n\" + first_q + \" (Yes/No)\"\n        # -------- normal chat ---------\n        return super().ask(user_msg, k=k)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T20:21:26.299136Z","iopub.execute_input":"2025-07-24T20:21:26.299418Z","iopub.status.idle":"2025-07-24T20:21:26.316282Z","shell.execute_reply.started":"2025-07-24T20:21:26.299399Z","shell.execute_reply":"2025-07-24T20:21:26.315589Z"}},"outputs":[],"execution_count":88},{"cell_type":"code","source":"bot = GameWebRAGMemoryChatbot(\n        exa_api_key=os.getenv(\"EXA_API_KEY\"),\n        retriever=retriever,\n        system_prompt=SYSTEM_PROMPT\n)\n\nuser_ans = \"Let's play 20 questions\"\n# user_ans = \"What is your name?\"\nquerries = []\n\nwhile user_ans != \"q\":\n    model_ques = bot.ask(user_ans)\n    print(model_ques)\n    user_ans = input(\"what is your answer?\")\n    \n    querries.append({\"Model Question\":model_ques,\"User answer\":user_ans})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T20:21:29.664628Z","iopub.execute_input":"2025-07-24T20:21:29.664889Z","iopub.status.idle":"2025-07-24T20:23:07.084324Z","shell.execute_reply.started":"2025-07-24T20:21:29.664869Z","shell.execute_reply":"2025-07-24T20:23:07.083576Z"}},"outputs":[{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Let's play 20 Questions! Think of a word; I'll guess in â‰¤20 yes/no questions.\n\nIs it an inanimate object? (Yes/No)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"what is your answer? no\n"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Person.\nIs it **person**? (Yes/No)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"what is your answer? no\n"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Is it a living organism? (Yes/No)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"what is your answer? yes\n"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"animal\nIs it **animal**? (Yes/No)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"what is your answer? no\n"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Does it have the ability to grow? (Yes/No)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"what is your answer? yes\n"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"plant\nIs it **plant**? (Yes/No)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"what is your answer? no\n"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Does it have a heart that beats? (Yes/No)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"what is your answer? yes\n"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Based on the given questions and answers, my one-word guess is: \"fungus\".\nIs it **based**? (Yes/No)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"what is your answer? No\n"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Does it have a circulatory system? (Yes/No)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"what is your answer? yes\n"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Based on the given questions and answers, my one-word guess is: \"animal.\" However, since you've provided a list of words that are not the answer, I will instead guess: \"fish.\"\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Based on the given questions and answers, my one-word guess is: \"animal.\" However, since you've provided a list of words that are not the answer, I will instead guess: \"fish.\"\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Based on the given questions and answers, my one-word guess is: \"animal.\" However, since you've provided a list of words that are not the answer, I will instead guess: \"fish.\"\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Based on the given questions and answers, my one-word guess is: \"animal.\" However, since you've provided a list of words that are not the answer, I will instead guess: \"fish.\"\nBased on the given questions and answers, my one-word guess is: \"animal.\" However, since you've provided a list of words that are not the answer, I will instead guess: \"fish.\"\nIs it **cat**? (Yes/No)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"what is your answer? q\n"}],"execution_count":89},{"cell_type":"code","source":"for q in querries:\n    print(\"Model question: \", q['Model Question'])\n    print(\"User answer: \", q['User answer'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T17:51:31.547630Z","iopub.execute_input":"2025-07-24T17:51:31.548188Z","iopub.status.idle":"2025-07-24T17:51:31.552734Z","shell.execute_reply.started":"2025-07-24T17:51:31.548167Z","shell.execute_reply":"2025-07-24T17:51:31.551914Z"}},"outputs":[{"name":"stdout","text":"Model question:  Let's play 20 Questions! Think of a word; I'll guess in â‰¤20 yes/no questions.\n\nIs it an inanimate object? (Yes/No)\nUser answer:  yes\nModel question:  Is it **chair**? (Yes/No)\nUser answer:  no\nModel question:  Is it made of metal? (Yes/No)\nUser answer:  yes\nModel question:  Is it **nail**? (Yes/No)\nUser answer:  no\nModel question:  Is it a type of appliance? (Yes/No)\nUser answer:  yes\nModel question:  Is it **can**? (Yes/No)\nUser answer:  no\nModel question:  Is it a large, inanimate metal object used for storing and cooking food? (Yes/No)\nUser answer:  no\nModel question:  Is it **oven**? (Yes/No)\nUser answer:  no\nModel question:  Is it a kitchen appliance made entirely of metal? (Yes/No)\nUser answer:  no\nModel question:  Is it **cat**? (Yes/No)\nUser answer:  no\nModel question:  Is it a metal desk or table used for working or eating? (Yes/No)\nUser answer:  no\nModel question:  Is it **dog**? (Yes/No)\nUser answer:  no\nModel question:  Is it a musical instrument made of metal? (Yes/No)\nUser answer:  no\nModel question:  Is it **tree**? (Yes/No)\nUser answer:  no\nModel question:  Is it an electrical appliance? (Yes/No)\nUser answer:  yes\nModel question:  Is it **battery**? (Yes/No)\nUser answer:  no\nModel question:  Is it an electrical gadget with a flat, rectangular shape? (Yes/No)\nUser answer:  yes\nModel question:  Is it **television**? (Yes/No)\nUser answer:  no\nModel question:  Is it a battery-powered electrical appliance? (Yes/No)\nUser answer:  yes\nModel question:  Is it **smartphone**? (Yes/No)\nUser answer:  yes\nModel question:  ğŸ‰ I guessed it! Thanks for playing.\nUser answer:  q\n","output_type":"stream"}],"execution_count":19}]}